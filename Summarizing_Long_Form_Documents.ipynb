{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarizing_Long-Form_Documents.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N0fGl-CnR1r2"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorenoSara/Summarizing-Long-Form-Document-with-Rich-Discourse-Information/blob/main/Summarizing_Long_Form_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "!pip install dgl\n",
        "!pip install dgl-cu101"
      ],
      "metadata": {
        "id": "ob_dOBNrDZTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "import sys\n",
        "import json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence, pad_sequence\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import dgl\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import os"
      ],
      "metadata": {
        "id": "aZ7P-f8K7s2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "OqIbqwy4R7Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "class Sentence():\n",
        "  def __init__(self, sentence, tokenized_sentence):\n",
        "    self.sentence = sentence \n",
        "    self.tokenized_sentence = tokenized_sentence\n",
        "    self.label = 0\n",
        "\n",
        "  def true_scoring(self, abstract):\n",
        "    score = rouge.get_scores(self.sentence, abstract.all_sentences) # abstract is a section\n",
        "    self.y_s = np.mean([score[0]['rouge-1']['f'], score[0]['rouge-2']['f'], score[0]['rouge-l']['f']])\n",
        "\n",
        "  def __len__(self):\n",
        "    return np.count_nonzero(self.tokenized_sentence)\n",
        "\n",
        "  def set_label(self):\n",
        "    self.label = 1\n",
        "\n",
        "\n",
        "class Section():\n",
        "  def __init__(self, title, sentences):\n",
        "    self.sentences = sentences\n",
        "    self.all_sentences = ' '.join([s.sentence for s in self.sentences])\n",
        "    self.title = title\n",
        "\n",
        "  def true_scoring(self, abstract):\n",
        "    self.y_S = rouge.get_scores(self.all_sentences, abstract.all_sentences)[0]['rouge-2']['r']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "\n",
        "\n",
        "class Document():\n",
        "  def __init__(self, sections, abstract):\n",
        "    self.sections = sections\n",
        "    self.abstract = abstract\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sections)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.sections[index]\n",
        "\n",
        "  def num_sentences(self):\n",
        "    return sum([len(s) for s in self.sections])"
      ],
      "metadata": {
        "id": "9gfmrTfjCim2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScientificPapaerDataset(Dataset): # map-style dataset\n",
        "    def __init__(self, datapath, max_sent_len, tokenizer):\n",
        "      \"\"\"\n",
        "      ScientificPapaerDataset reads the data from the file and builds the data structure.\n",
        "\n",
        "      :param datapath: path of the input document\n",
        "      :param max_sent_len: length to which each sentence is padded/truncated\n",
        "      :param tokenizer: tokenizer\n",
        "      \"\"\" \n",
        "      '''\n",
        "              format of each document:\n",
        "              { \n",
        "                  'article_id': str,\n",
        "                  'abstract_text': List[str],\n",
        "                  'article_text': List[str],\n",
        "                  'section_names': List[str],\n",
        "                  'sections': List[List[str]]\n",
        "              }\n",
        "      '''\n",
        "      super(ScientificPapaerDataset, self).__init__()\n",
        "\n",
        "      data = []\n",
        "      for line in open(datapath, 'r'):\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "      for doc in tqdm(data, desc='Building tokenizer'):\n",
        "        tokenizer.fit_on_texts(doc['section_names'])\n",
        "        for sec in doc['sections']:\n",
        "          tokenizer.fit_on_texts(sec)\n",
        "        tokenizer.fit_on_texts(doc['abstract_text'])\n",
        "        \n",
        "      \n",
        "      self.documents =[]\n",
        "      for doc in tqdm(data, desc='Scanning documents and building data structure'):\n",
        "\n",
        "        #Abstract\n",
        "        sentences = []\n",
        "        for abs_sent in doc['abstract_text']:\n",
        "          sentences.append(Sentence(abs_sent, pad_sequences(tokenizer.texts_to_sequences([abs_sent]), max_sent_len, padding='post')[0]))\n",
        "        abstract = Section(None, sentences)\n",
        "\n",
        "        #Sections\n",
        "        sections = []\n",
        "        for sec_title, sec in zip(doc['section_names'], doc['sections']):\n",
        "          title = Sentence(sec_title, pad_sequences(tokenizer.texts_to_sequences([sec_title]), max_sent_len, padding='post')[0])\n",
        "          if len(title) > 0: # avoid sections without title\n",
        "            if bool(re.match('^(?=.*[a-zA-Z])', sec[0])): # discard sections that start with an empty sentence\n",
        "              sentences = []\n",
        "              for sent in sec:\n",
        "                sentence = Sentence(sent, pad_sequences(tokenizer.texts_to_sequences([sent]), max_sent_len, padding='post')[0])\n",
        "                if len(sentence) > 0: # avoid empty sentences\n",
        "                  sentence.true_scoring(abstract)\n",
        "                  sentences.append(sentence)\n",
        "              section = Section(Sentence(sec_title, pad_sequences(tokenizer.texts_to_sequences([sec_title]), max_sent_len, padding='post')[0]), sentences)\n",
        "              section.true_scoring(abstract)\n",
        "              sections.append(section)\n",
        "        if len(sections) > 0: # avoid empty sections\n",
        "          self.documents.append(Document(sections, abstract))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      return self.documents[index]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.documents)\n"
      ],
      "metadata": {
        "id": "W8GjHWHWJ-Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "h5NCfkmhScHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CRDataLoader(DataLoader):\n",
        "  def __init__(self, dataset, batch_size, num_workers = 0, shuffle = True, max_sections_per_doc = 8, max_sentences_per_section = 18, max_sent_len = 36):\n",
        "    \"\"\"\n",
        "    CRDataLoader builds the dataloader for the Content Ranking Module.\n",
        "\n",
        "    :param dataset: dataset previously created\n",
        "    :param batch_size: size of the batch\n",
        "    :param num_workers: number of processes that generate batches in parallel.\n",
        "    :param shuffle: True to permute the indices of all samples\n",
        "    :param max_sections_per_doc: maximum number of sections contained in each doc\n",
        "    :param max_sentences_per_section: maximum number of sentences in each section\n",
        "    :param max_sent_len: maximum number of tokens in each sentence\n",
        "    :return: \n",
        "    \"\"\" \n",
        "    super(CRDataLoader,self).__init__(dataset, batch_size, num_workers=num_workers, shuffle=shuffle, collate_fn = self.my_collate_fn)\n",
        "    self.max_sent_len = max_sent_len\n",
        "    self.max_sentences_per_section = max_sentences_per_section\n",
        "    self.max_sections_per_doc = max_sections_per_doc\n",
        "\n",
        "\n",
        "  def my_collate_fn(self, batch): # no default collate because batch must contain tensors, numpy arrays, numbers, dicts or lists;\n",
        "    \"\"\"\n",
        "    my_collate_fn creates the data structure from each batch used in the training phase of the Content Ranking Module.\n",
        "\n",
        "    :param batch: the batch recived in input that contains instances of the Document class\n",
        "    :return dataset: a dictionary containg all the information used in the training\n",
        "    :return sections_importances: a 2D matrix containg the ground truth labels for each section\n",
        "    :return sentences_importances: a 3D matrix containg the ground truth labels for each sentence\n",
        "    \"\"\" \n",
        "    dataset = {\n",
        "        'section_titles': torch.zeros(len(batch),self.max_sections_per_doc,self.max_sent_len), # (doc, title, word)\n",
        "        'section_texts': torch.zeros(len(batch),self.max_sections_per_doc,self.max_sentences_per_section,self.max_sent_len), # (doc, section, sentence, word)\n",
        "        'sections_per_doc': torch.zeros(len(batch)), # (doc)\n",
        "        'sentences_per_section': torch.zeros(len(batch), self.max_sections_per_doc), # (doc, section)\n",
        "        'words_per_title': torch.zeros(len(batch), self.max_sections_per_doc), # (doc, section)\n",
        "        'words_per_sentence': torch.zeros(len(batch),self.max_sections_per_doc,self.max_sentences_per_section) # (doc, section, sentence)\n",
        "               }\n",
        "    sections_importances = torch.zeros(len(batch),self.max_sections_per_doc) # (doc, section)\n",
        "    sentences_importances = torch.zeros(len(batch),self.max_sections_per_doc,self.max_sentences_per_section) # (doc, section, sentence)\n",
        "\n",
        "    for doc_id, doc in enumerate(batch):\n",
        "      titles = [np.zeros(self.max_sent_len) for _ in range(self.max_sections_per_doc)] # shape (max_sections_per_doc, max_sent_len) => (title, word)\n",
        "      sections = [[np.zeros(self.max_sent_len) for _ in range(self.max_sentences_per_section)] for _ in range(self.max_sections_per_doc)] # shape (max_sections_per_doc, max_sentences_per_section, max_sent_len) => (section, sentence, word)\n",
        "      section_lengths = np.zeros(self.max_sections_per_doc)\n",
        "      title_lengths = np.zeros(self.max_sections_per_doc)\n",
        "      sentence_lengths =[np.zeros(self.max_sentences_per_section) for _ in range(self.max_sections_per_doc)]\n",
        "  \n",
        "      for sec_id, sec in enumerate(doc[:self.max_sections_per_doc]):\n",
        "          if len(sec) > self.max_sentences_per_section: \n",
        "            section_lengths[sec_id] = self.max_sentences_per_section\n",
        "          else :\n",
        "            section_lengths[sec_id] = len(sec)\n",
        "          if len(sec.title) > self.max_sentences_per_section: \n",
        "            title_lengths[sec_id] = self.max_sentences_per_section\n",
        "          elif len(sec.title) == 0:\n",
        "            continue\n",
        "          else :\n",
        "            title_lengths[sec_id] = len(sec.title)\n",
        "          sections_importances[doc_id][sec_id] = torch.tensor(sec.y_S)\n",
        "          titles[sec_id] = sec.title.tokenized_sentence\n",
        "          sentences = [np.zeros(self.max_sent_len) for _ in range(self.max_sentences_per_section) ] # shape (max_sentences_per_section, max_sent_len) => (sentence, word)\n",
        "          for sent_id, sent in enumerate(sec.sentences[:self.max_sentences_per_section]):\n",
        "              sentences_importances[doc_id][sec_id][sent_id]= torch.tensor(sent.y_s)\n",
        "              sentences[sent_id] = sent.tokenized_sentence\n",
        "              if len(sent) > self.max_sent_len: \n",
        "                sentence_lengths[sec_id][sent_id] = self.max_sent_len\n",
        "              else :\n",
        "                sentence_lengths[sec_id][sent_id] = len(sent)\n",
        "          sections[sec_id] = sentences\n",
        "      titles = torch.tensor(titles)\n",
        "      sections = torch.tensor(sections)\n",
        "      dataset['section_titles'][doc_id] = titles\n",
        "      dataset['section_texts'][doc_id] = sections\n",
        "      if len(doc) > self.max_sections_per_doc: \n",
        "        dataset['sections_per_doc'][doc_id] = self.max_sections_per_doc\n",
        "      else :\n",
        "        dataset['sections_per_doc'][doc_id] = len(doc)\n",
        "      dataset['sentences_per_section'][doc_id] = torch.tensor(section_lengths)\n",
        "      dataset['words_per_title'][doc_id] = torch.tensor(title_lengths)\n",
        "      dataset['words_per_sentence'][doc_id] = torch.tensor(sentence_lengths)\n",
        "    return dataset, sections_importances, sentences_importances "
      ],
      "metadata": {
        "id": "KKiHbcVSSe1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content ranking module"
      ],
      "metadata": {
        "id": "xftHQa2-sKD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding layer"
      ],
      "metadata": {
        "id": "N0fGl-CnR1r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "alTKZ97I2Ors"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class embedding_layer(nn.Module):\n",
        "  def __init__(self, tokenizer, embedding_dimension=300, device = 'cpu'):\n",
        "    \"\"\"\n",
        "      embedding_layer creates the embeddings for the Content Ranking Module starting from pretrained GloVe embeddings.\n",
        "\n",
        "      :param tokenizer: tokenizer previously instantiated\n",
        "      :param embedding_dimension:  embedding dimension, possible values: {50, 100, 200, 300}\n",
        "      :param device: cpu or cuda\n",
        "      \"\"\" \n",
        "    super(embedding_layer, self).__init__()\n",
        "    self.to(device)\n",
        "    self.d = device\n",
        "    embeddings_index = {}\n",
        "    with open(f\"glove.6B.{embedding_dimension}d.txt\") as f:\n",
        "        for line in f:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "    embedding_matrix = np.zeros((tokenizer.num_words, embedding_dimension))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if i < tokenizer.num_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            hits += 1\n",
        "        else:\n",
        "            misses += 1\n",
        "\n",
        "    #print(\"Converted %d words (%d misses)\" % (hits, misses)) # hits + misses = len(tokenizer.word_index)\n",
        "    embedding_matrix = torch.Tensor(embedding_matrix)\n",
        "    self.embedding = nn.Embedding(tokenizer.num_words, embedding_dimension)\n",
        "    self.embedding.weight = nn.Parameter(embedding_matrix)\n",
        "    self.embedding.weight.requires_grad = False\n",
        "\n",
        "  def forward(self, item):\n",
        "    item.to(self.d)\n",
        "    return self.embedding(item.to(torch.int32)).float()"
      ],
      "metadata": {
        "id": "BF6MKI8TDThT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Attention layer"
      ],
      "metadata": {
        "id": "Xfo7taN4FioW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordAttention(nn.Module):\n",
        "  def __init__(self, tokenizer, device, embedding_dimension=300, hidden_size=512):\n",
        "    super(WordAttention, self).__init__()\n",
        "    \"\"\"\n",
        "    WordAttention performs the word level attention mechanism.\n",
        "\n",
        "    :param tokenizer: tokenizer previously instantiated\n",
        "    :param device: cpu or cuda\n",
        "    :param embedding_dimension:  embedding dimension, possible values: {50, 100, 200, 300}, default 300\n",
        "    :param hidden_size: hidden size used for bidirectional LSTM layer, default 512\n",
        "    \"\"\" \n",
        "    self.embedder = embedding_layer(tokenizer, embedding_dimension)\n",
        "    self.BiLSTM = nn.LSTM(input_size=embedding_dimension, hidden_size=hidden_size, batch_first=True, bidirectional=True) # output dimenstion = 2*hidden_size because of bidirectionality\n",
        "    self.word_attention = nn.Linear(hidden_size*2, 2*hidden_size)\n",
        "    self.word_context_vector = nn.Linear(2*hidden_size, 1, bias = False)\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, sentences, words_per_sentence):\n",
        "    \"\"\"\n",
        "    WordAttention forward creates the sentence embeddings.\n",
        "\n",
        "    :param sentences: list of tokenized sentences\n",
        "    :param words_per_sentence: list with the number of words in each sentence\n",
        "    :return: sentence embeddings\n",
        "    \"\"\"\n",
        "    sentences = self.embedder(sentences) # (num_sentences, max_num_word_per_sentence, emb_dim)\n",
        "    packed_words = pack_padded_sequence(\n",
        "        sentences,\n",
        "        lengths= words_per_sentence.tolist(),\n",
        "        batch_first=True,\n",
        "        enforce_sorted=False\n",
        "    ) # returns a PackedSequence object, where 'data' is the flattened words (n_words, word_emb)\n",
        "    packed_words, _ = self.BiLSTM(packed_words) # returns a PackedSequence object, where 'data' is the output of the BiLSTM (n_words, 2 * hiddn_size)\n",
        "    att_w = self.word_attention(packed_words.data)  # (n_words, att_size)\n",
        "    \n",
        "    att_w = torch.tanh(att_w)\n",
        "    att_w = self.word_context_vector(att_w).squeeze(1)\n",
        "    \n",
        "    max_value = att_w.max()  # scalar, for numerical stability during exponent calculation\n",
        "    att_w = torch.exp(att_w - max_value)  # (n_words)\n",
        "\n",
        "    # Re-arrange as sentences by re-padding with 0s\n",
        "    att_w, _ = pad_packed_sequence(PackedSequence(data=att_w,\n",
        "                                                  batch_sizes=packed_words.batch_sizes,\n",
        "                                                  sorted_indices=packed_words.sorted_indices,\n",
        "                                                  unsorted_indices=packed_words.unsorted_indices),\n",
        "                                    batch_first=True)  # (n_sentences, max(words_per_sentence))\n",
        "          \n",
        "    \n",
        "    # Calculate softmax values as now words are arranged in their respective sentences\n",
        "    word_alphas = att_w / torch.sum(att_w, dim=1, keepdim=True)  # (n_sentences, max(words_per_sentence))\n",
        "\n",
        "    # Similarly re-arrange word-level BiLSTM outputs as sentences by re-padding with 0s\n",
        "    sentences, _ = pad_packed_sequence(packed_words,\n",
        "                                        batch_first=True)  # (n_sentences, max(words_per_sentence), 2 * hidden_size)\n",
        "\n",
        "    sentences = sentences * word_alphas.unsqueeze(2)  # (n_sentences, max(words_per_sentence), 2 * hidden_size)\n",
        "    sentences = sentences.sum(dim=1)  # (n_sentences, 2 * hidden_size)\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "cKjutQm7FJE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Attention layer\n",
        "Returns section representations and sentence scores"
      ],
      "metadata": {
        "id": "R5lgkVfMFnEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceAttention(nn.Module):\n",
        "  def __init__(self, tokenizer, device, embedding_dimension=300, hidden_size=512):\n",
        "    \"\"\"\n",
        "    SentenceAttention performs the sentence level attention mechanism.\n",
        "\n",
        "    :param tokenizer: tokenizer previously instantiated\n",
        "    :param device: cpu or cuda\n",
        "    :param embedding_dimension:  embedding dimension, possible values: {50, 100, 200, 300}, default 300\n",
        "    :param hidden_size: hidden size used for bidirectional LSTM layer, default 512\n",
        "    \"\"\" \n",
        "    super(SentenceAttention, self).__init__()\n",
        "    self.word_attention = WordAttention(tokenizer, device)\n",
        "    self.sentence_BiLSTM = nn.LSTM(input_size=2*hidden_size, hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
        "    self.sentence_attention = nn.Linear(hidden_size*2, 2*hidden_size)\n",
        "    self.sentence_context_vector = nn.Linear(2*hidden_size, 1, bias = False)\n",
        "    self.sentence_scores_layer = nn.Linear(2*hidden_size, 1)\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, data, sentences_per_sections, words_per_sentence):\n",
        "    \"\"\"\n",
        "    SentenceAttention forward creates the section embeddings.\n",
        "\n",
        "    :param data: matrix of tokens of each sentence, in each section\n",
        "    :param sentences_per_sections: list with the number of sentences in each section\n",
        "    :param words_per_sentence: list with the number of words in each sentence\n",
        "    :return sections: section embeddings\n",
        "    :return sentence_scores: a matrix with the ranking score assigned to each sentence for each section\n",
        "    \"\"\"\n",
        "    # from (num_non_null_sections, max_num_sentences_per_section) to (num_non_null_sencenteces)\n",
        "    packed_words_per_sent = pack_padded_sequence(words_per_sentence,\n",
        "                                      lengths=list(filter(lambda a: a != 0, sentences_per_sections.tolist())), #sentences_per_sections.tolist(),\n",
        "                                      batch_first=True,\n",
        "                                      enforce_sorted=False)\n",
        "    \n",
        "    # from (num_non_null_sections, max_num_sentences_per_section, max_num_words_per_sentence) to (num_non_null_sentences, max_num_words_per_sentence)\n",
        "    packed_sections_by_sentence = pack_padded_sequence(data,\n",
        "                                      lengths=sentences_per_sections.tolist(),\n",
        "                                      batch_first=True,\n",
        "                                      enforce_sorted=False)\n",
        "    \n",
        "    sentences = self.word_attention(packed_sections_by_sentence.data, packed_words_per_sent.data)\n",
        "\n",
        "    packed_sentences, _ = self.sentence_BiLSTM(PackedSequence(data=sentences,\n",
        "                                                           batch_sizes=packed_sections_by_sentence.batch_sizes,\n",
        "                                                           sorted_indices=packed_sections_by_sentence.sorted_indices,\n",
        "                                                           unsorted_indices=packed_sections_by_sentence.unsorted_indices))\n",
        "  \n",
        "    # Find attention vectors by applying the attention linear layer on the output of the sentence BiLSTM\n",
        "    att_s = self.sentence_attention(packed_sentences.data)  # (n_non_null_sentences, 2*hidden_size)\n",
        "    att_s = torch.tanh(att_s)  # (n_non_null_sentences, 2*hidden_size)\n",
        "    # Take the dot-product of the attention vectors with the context vector (i.e. parameter of linear layer)\n",
        "    att_s = self.sentence_context_vector(att_s).squeeze(1)  # (n_non_null_sentences)\n",
        "\n",
        "    max_value = att_s.max()  # scalar, for numerical stability during exponent calculation\n",
        "    att_s = torch.exp(att_s - max_value)  # (n_non_null_sentences)\n",
        "\n",
        "    # Re-arrange as sections by re-padding with 0s\n",
        "    att_s, _ = pad_packed_sequence(PackedSequence(data=att_s,\n",
        "                                                  batch_sizes=packed_sentences.batch_sizes,\n",
        "                                                  sorted_indices=packed_sentences.sorted_indices,\n",
        "                                                  unsorted_indices=packed_sentences.unsorted_indices),\n",
        "                                    batch_first=True)  # (n_non_null_sections, max(sentences_per_document))\n",
        "\n",
        "    # Calculate softmax values as now sentences are arranged in their respective sections\n",
        "    sentence_alphas = att_s / torch.sum(att_s, dim=1, keepdim=True)  # (n_non_null_sections, max_sentences_per_document)\n",
        "\n",
        "    # Similarly re-arrange sentence-level BiLSTM outputs as sections by re-padding with 0s (sentences -> sections)\n",
        "    sections, _ = pad_packed_sequence(packed_sentences,\n",
        "                                        batch_first=True)  # (n_non_null_sections, max(sentences_per_document), 2 * hidden_size)\n",
        "    \n",
        "    \n",
        "    sentence_scores = torch.sigmoid(self.sentence_scores_layer(packed_sentences.data)) # shape (num_non_null_sentences, 1)\n",
        "\n",
        "    sentence_scores, _ = pad_packed_sequence(PackedSequence(data=sentence_scores,\n",
        "                                                  batch_sizes=packed_sentences.batch_sizes,\n",
        "                                                  sorted_indices=packed_sentences.sorted_indices,\n",
        "                                                  unsorted_indices=packed_sentences.unsorted_indices),\n",
        "                                    batch_first=True)# (num_non_null_sections, max_sentences_per_section, 1)\n",
        "                                    \n",
        "    # Find section embeddings\n",
        "    sections = sections * sentence_alphas.unsqueeze(2)  # (n_non_null_sections, max(sentences_per_document), 2 * hidden_size)\n",
        "    sections = sections.sum(dim=1)  # (n_non_null_sections, 2 * hidden_size)\n",
        "\n",
        "    return sections, sentence_scores\n"
      ],
      "metadata": {
        "id": "S3sM3le1s4uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete module\n",
        "Computes section and sentence scores"
      ],
      "metadata": {
        "id": "YtpuvX5gF3E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentRankingModule(nn.Module):\n",
        "  def __init__(self, tokenizer, device, embedding_dimension=300, hidden_size=512):\n",
        "    \"\"\"\n",
        "    Content Ranking Module ranks each sentence and each section.\n",
        "\n",
        "    :param tokenizer: tokenizer previously instantiated\n",
        "    :param device: cpu or cuda\n",
        "    :param embedding_dimension:  embedding dimension, possible values: {50, 100, 200, 300}, default 300\n",
        "    :param hidden_size: hidden size used for bidirectional LSTM layer, default 512\n",
        "    \"\"\" \n",
        "    super(ContentRankingModule, self).__init__()\n",
        "\n",
        "    self.title_attention = WordAttention(tokenizer, device)\n",
        "    self.section_and_sentence_attention = SentenceAttention(tokenizer, device)\n",
        "    self.section_and_title_attention = nn.Linear(hidden_size*2, 2*hidden_size)\n",
        "    self.section_and_title_context_vector = nn.Linear(2*hidden_size, 1, bias = False)\n",
        "    self.section_scores_layer = nn.Linear(2*hidden_size, 1)\n",
        "    self.to(device)\n",
        "    \n",
        "\n",
        "  def forward(self, data): \n",
        "    \"\"\"\n",
        "    Content Ranking Module forward call.\n",
        "\n",
        "    :param data: the dictionary returned by the dataloader\n",
        "    :return section_scores: a matrix containg rank of each section\n",
        "    :return sentence_scores: a matrix containing the ranking of each sentence of each section\n",
        "    \"\"\"\n",
        "    # from (batch, max_num_section, max_sent_len) to (num_non_null_sections, max_sent_len)  \n",
        "    packed_titles = pack_padded_sequence(data['section_titles'],\n",
        "                                         lengths=data['sections_per_doc'].tolist(),\n",
        "                                         batch_first=True,\n",
        "                                         enforce_sorted=False)\n",
        "    \n",
        "    # from (batch, max_num_section) to (num_non_null_sections)\n",
        "    packed_words_per_title = pack_padded_sequence(data['words_per_title'],\n",
        "                                         lengths=data['sections_per_doc'].tolist(),\n",
        "                                         batch_first=True,\n",
        "                                         enforce_sorted=False)\n",
        "    \n",
        "    titles = self.title_attention(packed_titles.data, packed_words_per_title.data) # shape (num_non_null_section, hidden_size*2)\n",
        "    \n",
        "    titles, _ = pad_packed_sequence(PackedSequence(data=titles,\n",
        "                                                      batch_sizes=packed_titles.batch_sizes,\n",
        "                                                      sorted_indices=packed_titles.sorted_indices,\n",
        "                                                      unsorted_indices=packed_titles.unsorted_indices),\n",
        "                                       batch_first=True) # (batch, max_num_section, hidden_size*2)\n",
        "    \n",
        "    # CREATE INDEX\n",
        "    # from (batch, max_num_sections, max_num_sentences_per_section) to (num_non_null_sections, max_num_sentences_per_section)\n",
        "    packed_words_per_sentence = pack_padded_sequence(data['words_per_sentence'],\n",
        "                                      lengths=data['sections_per_doc'].tolist(),\n",
        "                                      batch_first=True,\n",
        "                                      enforce_sorted=False)\n",
        "    \n",
        "    # from (batch, max_num_sections) to (num_non_null_sections)\n",
        "    packed_sentences_per_section = pack_padded_sequence(data['sentences_per_section'],\n",
        "                                      lengths=data['sections_per_doc'].tolist(),\n",
        "                                      batch_first=True,\n",
        "                                      enforce_sorted=False)\n",
        "    \n",
        "    # PACK DATA\n",
        "    # from (batch, max_num_sections_per_doc, max_num_sentences_per_section, max_num_words_per_sentence) to (num_non_null_sections, max_num_sentences_per_section, max_num_words_per_sentence)\n",
        "    packed_sections = pack_padded_sequence(data['section_texts'],\n",
        "                                        lengths=data['sections_per_doc'].tolist(),\n",
        "                                        batch_first=True,\n",
        "                                        enforce_sorted=False)\n",
        "    \n",
        "    sections, sentence_scores = self.section_and_sentence_attention(packed_sections.data, packed_sentences_per_section.data, packed_words_per_sentence.data)\n",
        "    # sections: (num_non_null_sections, 2 * hidden_size)\n",
        "    # sentence_scores: (num_non_null_sections, max_sentences_per_section, 1)\n",
        "\n",
        "    sentence_scores, _ = pad_packed_sequence(PackedSequence(data=sentence_scores,\n",
        "                                                  batch_sizes=packed_sections.batch_sizes,\n",
        "                                                  sorted_indices=packed_sections.sorted_indices,\n",
        "                                                  unsorted_indices=packed_sections.unsorted_indices),\n",
        "                                    batch_first=True) # (batch, max_num_sections_per_doc, max_sentences_per_section, 1)\n",
        "    sections, _ = pad_packed_sequence(PackedSequence(data=sections,\n",
        "                                                  batch_sizes=packed_sentences_per_section.batch_sizes,\n",
        "                                                  sorted_indices=packed_sentences_per_section.sorted_indices,\n",
        "                                                  unsorted_indices=packed_sentences_per_section.unsorted_indices),\n",
        "                                    batch_first=True) # (batch, max_num_sections_per_doc, hidden_size*2)\n",
        "\n",
        "    sections = sections.unsqueeze(2) # shape(batch, max_num_sections_per_doc, 1, hidden_size*2)\n",
        "    titles = titles.unsqueeze(2) # shape(batch, max_num_sections_per_doc, 1, hidden_size*2)\n",
        "    sections_with_titles = torch.stack((titles, sections), dim = 2).squeeze() # \"vertical\" stack of 2 4D matrices\n",
        "    packed_sections_with_titles = pack_padded_sequence(sections_with_titles,\n",
        "                                         lengths=data['sections_per_doc'].tolist(),\n",
        "                                         batch_first=True,\n",
        "                                         enforce_sorted=False) # (num_non_null_sections, 2, hidden_size*2)\n",
        "    att_S = self.section_and_title_attention(packed_sections_with_titles.data)\n",
        "    att_S = torch.tanh(att_S)\n",
        "    att_S = self.section_and_title_context_vector(att_S).squeeze(1)\n",
        "    max_value = att_S.max()\n",
        "    att_S = torch.exp(att_S - max_value)\n",
        "    att_S, _ = pad_packed_sequence(PackedSequence(data=att_S,\n",
        "                                                      batch_sizes=packed_sections_with_titles.batch_sizes,\n",
        "                                                      sorted_indices=packed_sections_with_titles.sorted_indices,\n",
        "                                                      unsorted_indices=packed_sections_with_titles.unsorted_indices),\n",
        "                                       batch_first=True)\n",
        "    section_alphas = att_S / torch.sum(att_S, dim=1, keepdim=True) # shape (batch, max_num_sections_per_doc, 2, 1)\n",
        "    sections, _ = pad_packed_sequence(packed_sections_with_titles,\n",
        "                                           batch_first=True) # shape(batch, max_num_sections_per_doc, 2, hidden_size*2) 2 because of vertical stack of title and content representations\n",
        "    \n",
        "    sections = sections * section_alphas \n",
        "    sections = sections.sum(dim=2) # final shape (batch, max_num_sections_per_doc, hidden_size*2)\n",
        "    \n",
        "    packed_sections = pack_padded_sequence(sections,\n",
        "                                        lengths=data['sections_per_doc'].tolist(),\n",
        "                                        batch_first=True,\n",
        "                                        enforce_sorted=False) # shape (num_non_null_sections, 2*hidden_size)\n",
        "\n",
        "    section_scores = torch.sigmoid(self.section_scores_layer(packed_sections.data)) # shape (num_non_null_sections, 1)\n",
        "\n",
        "    section_scores, _ = pad_packed_sequence(PackedSequence(data=section_scores,\n",
        "                                                  batch_sizes=packed_sections.batch_sizes,\n",
        "                                                  sorted_indices=packed_sections.sorted_indices,\n",
        "                                                  unsorted_indices=packed_sections.unsorted_indices),\n",
        "                                    batch_first=True) # (batch, max_num_sections_per_doc, 1)\n",
        "    \n",
        "    return section_scores.squeeze(), sentence_scores.squeeze() # become 2D and 3D matrices respectively\n",
        "\n"
      ],
      "metadata": {
        "id": "iDeAqSXbsOWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss"
      ],
      "metadata": {
        "id": "ypmDsgIZGH5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CRLoss(nn.Module):\n",
        "  def __init__(self, device):\n",
        "      \"\"\"\n",
        "      CRLoss computes the Binary Cross Entropy loss betwee the ranking scores and the ground truth labels.\n",
        "\n",
        "      :param device: cpu or cuda\n",
        "      \"\"\"\n",
        "      super(CRLoss, self).__init__()\n",
        "      self.section_loss = nn.BCELoss()\n",
        "      self.sentence_loss = nn.BCELoss()\n",
        "      self.to(device)\n",
        "\n",
        "  def forward(self, sections_importance, sentences_importance, sections_gold, sentences_gold):\n",
        "      \"\"\"\n",
        "      CRLoss forward call.\n",
        "\n",
        "      :param sections_importance: matrix of the generated section ranking\n",
        "      :param sentences_importance: matrix of the generated sentence ranking\n",
        "      :param sections_gold: matrix of the ground truth section scores\n",
        "      :param sentences_gold: matrix of the ground truth sentence scores\n",
        "      :return: sum of the losses computed on section and sentences\n",
        "      \"\"\"\n",
        "      # if in a batch the maximum number of sections and sentences is lower than the maximum value fixed, \n",
        "      # using pad_packed_sequnce the dimentions returned will be equal to the maximal dimentions of the batch. \n",
        "      # Therefore it is necessary to properly slice the ground truth labels\n",
        "      loss1 = self.section_loss(sections_importance, sections_gold[:, :sections_importance.shape[1]]) \n",
        "      loss2 = self.sentence_loss(sentences_importance, sentences_gold[:, :sentences_importance.shape[1], :sentences_importance.shape[2]])\n",
        "      return torch.add(loss1,loss2)"
      ],
      "metadata": {
        "id": "STWcGxSnGJtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training of content ranking module"
      ],
      "metadata": {
        "id": "Wu-Ye85TIrNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 25\n",
        "eval_batch_size = 10\n",
        "lr = 1e-4\n",
        "epochs = 5\n",
        "workers = 2\n",
        "m = 2\n",
        "n = 4\n",
        "train_datapath = '/content/drive/MyDrive/arx_pub-dataset/train_subset.txt'\n",
        "eval_datapath = '/content/drive/MyDrive/arx_pub-dataset/val_subset.txt'\n",
        "CR_model_path = '/content/drive/MyDrive/CR_model_arx_pub.pkl'\n",
        "max_sent_len = 36\n",
        "tokenizer = Tokenizer(num_words=50000) # keep only the 50000 more frequent words"
      ],
      "metadata": {
        "id": "uEerTmbJhT3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.setrecursionlimit(500000)\n",
        "documents = ScientificPapaerDataset(train_datapath, max_sent_len, tokenizer)\n",
        "eval_docs = ScientificPapaerDataset(eval_datapath, max_sent_len, tokenizer) \n",
        "\n",
        "content_ranking_dataloader = CRDataLoader(documents, batch_size=batch_size, num_workers=workers)\n",
        "CR_eval_dataloader = CRDataLoader(eval_docs, batch_size=eval_batch_size, num_workers=workers)"
      ],
      "metadata": {
        "id": "mKwwFxmZrjYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ContentRankingModule(tokenizer=tokenizer, device=device)\n",
        "model.to(device)\n",
        "\n",
        "CR_loss = CRLoss(device)\n",
        "CR_loss.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "2k2jN0JzlK9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "aVD9GvBN75hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_path):\n",
        "    \"\"\"Save model.\"\"\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_model(model, model_path, use_cuda=True):\n",
        "    \"\"\"Load model.\"\"\"\n",
        "    map_location = 'cpu'\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "        map_location = 'cuda:0'\n",
        "    model.load_state_dict(torch.load(model_path, map_location))\n",
        "    return model"
      ],
      "metadata": {
        "id": "uA-ewwTb8m51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_eval_loss = np.inf\n",
        "for epoch in range(epochs):\n",
        "  # Training\n",
        "  training_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch, (dataset, sections_reference_importances, sentences_reference_importances) in enumerate(content_ranking_dataloader):\n",
        "     # put anything on cuda if available\n",
        "    for k in dataset.keys():\n",
        "      dataset[k] = dataset[k].to(device)\n",
        "    sections_reference_importances = sections_reference_importances.to(device)\n",
        "    sentences_reference_importances = sentences_reference_importances.to(device)\n",
        "    \n",
        "    # forward call of the model\n",
        "    sections_scores, sentences_scores = model(dataset)\n",
        "\n",
        "    loss = CR_loss(sections_scores, sentences_scores, sections_reference_importances, sentences_reference_importances)\n",
        "\n",
        "    training_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {batch}/{epochs}. Training loss: {training_loss:.3f}.')\n",
        "\n",
        "\n",
        "  # Evaluation\n",
        "  model.eval()\n",
        "  eval_loss = 0\n",
        "  for eval_batch, (eval, sections_reference_importances_eval, sentences_reference_importances_eval) in enumerate(CR_eval_dataloader):\n",
        "    for k in eval.keys():\n",
        "      eval[k] = eval[k].to(device)\n",
        "    sections_reference_importances_eval = sections_reference_importances_eval.to(device)\n",
        "    sentences_reference_importances_eval = sentences_reference_importances_eval.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      sections_scores_eval, sentences_scores_eval = model(eval)\n",
        "\n",
        "    eval_batch_loss = CR_loss(sections_scores_eval, sentences_scores_eval, sections_reference_importances_eval, sentences_reference_importances_eval)\n",
        "    eval_loss += eval_batch_loss.item()\n",
        "  print(\"Evaluation loss: \", eval_loss)\n",
        "    \n",
        "  if eval_loss < best_eval_loss: # save the model that reaches the lowest loss\n",
        "    print(\"Saving best model\")\n",
        "    best_eval_loss = eval_loss\n",
        "    save_model(model, CR_model_path)"
      ],
      "metadata": {
        "id": "RmeCYavgm9FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "zEQhCrt0IpK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, data, batch_size, workers, max_num_sec = 8, max_num_sent = 18):\n",
        "  \"\"\"\n",
        "  predict applies the trained Content Ranking Module on the data recived in input.\n",
        "\n",
        "  :param model: Content Ranking Module already trained\n",
        "  :param data: data to feed to the internal data loader\n",
        "  :param batch_size: batch size used by the internal data loader\n",
        "  :param max_num_sec: maximum number of sections for each document, default 8\n",
        "  :param max_num_sent: maximum number of sentences for each section, default 18\n",
        "  :return: 2 matrices, one containing the genreated scores for the sections and one for those related to the sentences\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  all_section_scores = []\n",
        "  all_sentence_scores = []\n",
        "  \n",
        "  dataloader = CRDataLoader(data, batch_size=batch_size, num_workers=workers, shuffle = False)\n",
        "  for _ , (dataset, _, _) in enumerate(dataloader):\n",
        "    # because pad_packed_sequence returns the dimensions of the longest sequence in the batch, \n",
        "    # thus there can be a dimensional mismatch between the retruned matrices of different batches \n",
        "    empy_sec_score = torch.zeros(batch_size, max_num_sec) \n",
        "    empty_sent_score = torch.zeros(batch_size, max_num_sec, max_num_sent)\n",
        "    for k in dataset.keys():\n",
        "      dataset[k] = dataset[k].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      sections_scores, sentences_scores = model(dataset)\n",
        "\n",
        "    empy_sec_score[:sections_scores.shape[0], :sections_scores.shape[1]] = sections_scores\n",
        "    empty_sent_score[:sentences_scores.shape[0], :sentences_scores.shape[1], :sentences_scores.shape[2]] = sentences_scores\n",
        "    all_section_scores.append(empy_sec_score[:sections_scores.shape[0]]) # preserves the len of the batch\n",
        "    all_sentence_scores.append(empty_sent_score[:sentences_scores.shape[0]])\n",
        "  \n",
        "  all_section_scores = torch.cat(all_section_scores, 0)\n",
        "  all_sentence_scores = torch.cat(all_sentence_scores, 0)\n",
        "  return all_section_scores, all_sentence_scores\n"
      ],
      "metadata": {
        "id": "bQHlomPH-IWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the model to the training and evaluation data sets"
      ],
      "metadata": {
        "id": "PRhnNkWphdci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(ContentRankingModule(tokenizer=tokenizer, device=device), CR_model_path) # load the previously saved best model\n",
        "all_section_scores, all_sentence_scores= predict(model, documents, batch_size, workers)\n",
        "eval_section_scores, eval_sentence_scores = predict(model, eval_docs, batch_size, workers)"
      ],
      "metadata": {
        "id": "EaIzRtdZE1aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Digested dataset"
      ],
      "metadata": {
        "id": "SVkRtNMkCVvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top_sections_and_sentences(section_scores, sentence_scores, m, n):\n",
        "  \"\"\"\n",
        "  select_top_sections_and_sentences selects the indices of the best scoring sections and sentences, \n",
        "  maintaining the order in which their respective document.\n",
        "\n",
        "  :param section_scores: Content Ranking Module scores for the sections\n",
        "  :param sentence_scores: Content Ranking Module scores for the sentences\n",
        "  :param m: number of sections to select\n",
        "  :param n: number of sentences to select\n",
        "  :return: 2 lists of ids, the first are the selected sections, the second are the selected sentences\n",
        "  \"\"\"\n",
        "  sec_sort_ids = torch.argsort(section_scores, dim=1, descending = True)[:, :m].sort().values\n",
        "  sent_sort_ids = torch.argsort(sentence_scores, dim=2, descending = True)[:, :, :n].sort().values\n",
        "\n",
        "  return sec_sort_ids, sent_sort_ids"
      ],
      "metadata": {
        "id": "5vKONPQjXCZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_boundary_distance(l, pos): \n",
        "  \"\"\"\n",
        "  compute_boundary_distance computes the boundary distance feature for each sentence with respect to the corresponding section.\n",
        "\n",
        "  :param l: number of sentence in the section\n",
        "  :param pos: sentence position of the sentence in the section\n",
        "  :return: integer indicating the distance from the boundaries of the section\n",
        "  \"\"\"\n",
        "  dist = abs((pos - (l/2)))/(l/2)\n",
        "  return dist"
      ],
      "metadata": {
        "id": "7_Eh7U3rOOkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DigestedDataset(Dataset): # map-style dataset\n",
        "  def __init__(self, data, top_sections_ids, top_sentences_ids, m = 2, n = 4, n_best_sent_per_abs_sent = 2, disable_bound_dist = False):\n",
        "    \"\"\"\n",
        "    DigestedDataset creates the digested documenets.\n",
        "\n",
        "    :param data: list of Document intances\n",
        "    :param top_sections_ids: best ranked section ids outputted by the select_top_sections_and_sentences function\n",
        "    :param top_sentences_ids: best ranked section ids outputted by the select_top_sections_and_sentences function\n",
        "    :param m: number of sections to select, default 2\n",
        "    :param n: number of sentences to select, defalut 4\n",
        "    :param n_best_sent_per_abs_sent: number of digested document sentences to be selected accordingly to their \n",
        "                                     average ROUGE F1 score computed against each abstract sentence\n",
        "    :param disable_bound_dist: default False, if True disables the boundary distance contribution, useful for the ablation study\n",
        "    \"\"\"\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.k = n_best_sent_per_abs_sent\n",
        "    super(DigestedDataset, self).__init__()\n",
        "    self.documents = []\n",
        "    for doc_id, doc in enumerate(tqdm(data, desc=\"Reorganizing dataset\")):\n",
        "      sections = []\n",
        "      for sec_id in top_sections_ids[doc_id]:\n",
        "        if sec_id >= len(doc):\n",
        "          continue # to handle documents with less than the defined maximum number of sections\n",
        "        sentences = []\n",
        "        for i in top_sentences_ids[doc_id][sec_id].tolist():\n",
        "          if i >= len(doc[sec_id].sentences):\n",
        "            continue# to handle sections with less than the defined maximum number of sentences\n",
        "          sentences.append(doc[sec_id].sentences[i])\n",
        "        sections.append(Section(None, sentences))\n",
        "      sections = self.define_labels(doc.abstract, sections)\n",
        "      self.documents.append(Document(sections, doc.abstract))\n",
        "      self.dis_bd = disable_bound_dist\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    G = self.createGraph(self.documents[index])\n",
        "    return G, index\n",
        "\n",
        "  def get_doc(self, index):\n",
        "    return self.documents[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.documents)\n",
        "\n",
        "  def define_labels(self, abstract, sections): # defines the ground truth labels of each sentence\n",
        "    for abs_sent in abstract.sentences:\n",
        "      sentence_scores = torch.zeros(self.m * self.n)\n",
        "      for sec_id, sec in enumerate(sections):\n",
        "        for sent_id, sent in enumerate(sec.sentences):\n",
        "          score = rouge.get_scores(sent.sentence, abs_sent.sentence) # abstract is a section\n",
        "          score = np.mean([score[0]['rouge-1']['f'], score[0]['rouge-2']['f'], score[0]['rouge-l']['f']])\n",
        "          sentence_scores[sent_id+sec_id*self.n] = score\n",
        "      best_indices = sentence_scores.argsort(descending = True)[:self.k]\n",
        "      for index in best_indices:\n",
        "        if index//self.n < len(sections):\n",
        "          if index%self.n < len(sections[index//self.n].sentences):\n",
        "            sections[index//self.n].sentences[index%self.n].set_label()\n",
        "    return sections\n",
        "\n",
        "  def AddWordNode(self, G, document):\n",
        "    wid2nid = {} # dictionary mapping word ids (keys) to node ids (values)\n",
        "    nid = 0\n",
        "    for sec in document.sections:\n",
        "      for sent in sec.sentences:\n",
        "          for wid in sent.tokenized_sentence:\n",
        "              if wid not in wid2nid.keys() and wid != 0:\n",
        "                  wid2nid[wid] = nid\n",
        "                  nid += 1\n",
        "\n",
        "    w_nodes = len(wid2nid)\n",
        "\n",
        "    G.add_nodes(w_nodes)\n",
        "    G.set_n_initializer(dgl.init.zero_initializer)\n",
        "    G.ndata[\"id\"] = torch.LongTensor(list(wid2nid.keys()))\n",
        "    G.ndata[\"semantic_type\"] = torch.zeros(w_nodes) # semantic_type of word nodes = 0\n",
        "\n",
        "    return wid2nid\n",
        "\n",
        "  def MapSent2Sec(self, doc, num_sentences):\n",
        "    sent2sec = {} # dictionary mapping the sentence numbers (keys) to the corresponding section number (values)\n",
        "    sentNo = 0\n",
        "    for i, sec in enumerate(doc.sections):\n",
        "      for j in range(len(sec)):\n",
        "        sent2sec[sentNo] = i\n",
        "        sentNo += 1\n",
        "        if sentNo >= num_sentences:\n",
        "          return sent2sec\n",
        "    return sent2sec\n",
        "\n",
        "\n",
        "  def createGraph(self, document):\n",
        "    # builds a heterogeneous graph for each digested document\n",
        "    G = dgl.DGLGraph()\n",
        "    wid2nid = self.AddWordNode(G, document)\n",
        "    w_nodes = len(wid2nid)\n",
        "\n",
        "    N = document.num_sentences()\n",
        "    G.add_nodes(N)\n",
        "    G.ndata[\"semantic_type\"][w_nodes:] = torch.ones(N) # semantic_type of sentence nodes = 1\n",
        "    sentid2nid = [i + w_nodes for i in range(N)]\n",
        "    ws_nodes = w_nodes + N\n",
        "    \n",
        "    sent2sec = self.MapSent2Sec(document, N)\n",
        "    sec_num = len(set(sent2sec.values()))\n",
        "    G.add_nodes(sec_num)\n",
        "    G.ndata[\"semantic_type\"][ws_nodes:] = torch.ones(sec_num) * 2 # semantic_type of section nodes = 2\n",
        "    secid2nid = [i + ws_nodes for i in range(sec_num)]\n",
        "\n",
        "    sent_id = 0 # all the sentences of the different sections have consecutive ids\n",
        "    for sec_id, sec in enumerate(document.sections):\n",
        "      secid = sent2sec[sent_id]\n",
        "      secnid = secid2nid[secid] # node id of the section containit the considered sentence\n",
        "      for sent_count, sent in enumerate(sec.sentences):\n",
        "        c = Counter(sent.tokenized_sentence)\n",
        "        sent_nid = sentid2nid[sent_id] # node id of the considered sentence\n",
        "        G.nodes[sent_nid].data[\"label\"] = torch.LongTensor([sent.label])\n",
        "        G.nodes[sent_nid].data[\"words\"] = torch.LongTensor([sent.tokenized_sentence]) # list of token ids\n",
        "        G.nodes[sent_nid].data[\"num_words\"] = torch.LongTensor([len(sent)])\n",
        "\n",
        "        ###########################################   BOUNDARY DISTANCE    ############################################\n",
        "        if self.dis_bd == True:\n",
        "          G.nodes[sent_nid].data[\"boundary_dist\"] = torch.LongTensor([1]) # all 1 if boundary distance feature is disabled\n",
        "        else: \n",
        "          G.nodes[sent_nid].data[\"boundary_dist\"] = torch.LongTensor([compute_boundary_distance(len(sec), sent_count)])\n",
        "        ############################################################################################################### \n",
        "\n",
        "        G.add_edges(sent_nid, secnid,\n",
        "                            data={\"edge_type\": torch.Tensor([4])}) # intra-section sentence2section: 4\n",
        "        for other_sec_nid in secid2nid:\n",
        "          if other_sec_nid != secnid:\n",
        "            G.add_edges(other_sec_nid, sent_nid,\n",
        "                            data={\"edge_type\": torch.Tensor([3])}) # cross-section section2sentence: 3\n",
        "        for wid, cnt in c.items():\n",
        "            if wid in wid2nid.keys():\n",
        "                # w2s s2w\n",
        "                G.add_edges(wid2nid[wid], sent_nid,\n",
        "                            data={\"edge_type\": torch.Tensor([0])}) # word2sentenc: 0\n",
        "                G.add_edges(sent_nid, wid2nid[wid],\n",
        "                            data={\"edge_type\": torch.Tensor([1])}) # sentence2word: 1\n",
        "        sent_id += 1\n",
        "\n",
        "      \n",
        "      for (sent_nid1, sent_nid2) in combinations([sentid2nid[sid] for sid,Sid in sent2sec.items() if Sid == sec_id], 2):\n",
        "        G.add_edges(sent_nid1, sent_nid2, data={\"edge_type\": torch.Tensor([2])})# intra-section sentence2sentence: 2\n",
        "        G.add_edges(sent_nid2, sent_nid1, data={\"edge_type\": torch.Tensor([2])})\n",
        "\n",
        "    for (sec_nid1, sec_nid2) in combinations(secid2nid, 2):\n",
        "      G.add_edges(sec_nid1, sec_nid2,\n",
        "                          data={\"edge_type\": torch.Tensor([5])}) # section2section: 5\n",
        "      G.add_edges(sec_nid2, sec_nid1,\n",
        "                          data={\"edge_type\": torch.Tensor([5])}) # section2section: 5\n",
        "\n",
        "    return G"
      ],
      "metadata": {
        "id": "9IUOXWudHUEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_sections_ids, top_sentences_ids = select_top_sections_and_sentences(all_section_scores, all_sentence_scores, 2, 4)\n",
        "eval_top_sections_ids, eval_top_sentences_ids = select_top_sections_and_sentences(eval_section_scores, eval_sentence_scores, 2, 4)"
      ],
      "metadata": {
        "id": "BS-gwUu1YPbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digested_docs = DigestedDataset(documents, top_sections_ids, top_sentences_ids, m, n)\n",
        "eval_digested_docs = DigestedDataset(eval_docs, eval_top_sections_ids, eval_top_sentences_ids, m, n)"
      ],
      "metadata": {
        "id": "lFdXWluWJF2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extractive summarization dataloader"
      ],
      "metadata": {
        "id": "FlmT0LKQDrBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_collate_fn(samples):\n",
        "  '''\n",
        "  graph_collate_fn creates batches of graphs\n",
        "  \n",
        "  :param samples: (G, index)\n",
        "  :return: (batched graph, index)\n",
        "  '''\n",
        "  graphs, index = map(list, zip(*samples)) # graphs is a list of graphs, index is a list of the corresponding indices, both of len batch_size\n",
        "  batched_graph = dgl.batch(graphs)\n",
        "  return batched_graph, index"
      ],
      "metadata": {
        "id": "cGhAv7tFA3HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extractive summarization module"
      ],
      "metadata": {
        "id": "6cfzvW8pb5qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create $H^{0}_{w|s|S}$"
      ],
      "metadata": {
        "id": "9ncaW36jF_wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class sentence_cnn_encoder(nn.Module):\n",
        "  def __init__(self, embedding_layer, device = 'cpu', embedding_dimension = 300):\n",
        "    \"\"\"\n",
        "    sentence_cnn_encoder creates sentence embeddings using a CNN.\n",
        "\n",
        "    :param embedding_layer: layer that creates sentence initial embeddings\n",
        "    :param device: cpu or cuda\n",
        "    :param embedding_dimension: final embedding size, default 300\n",
        "    \"\"\"\n",
        "    super(sentence_cnn_encoder, self).__init__()\n",
        "    self.sent_embedder = embedding_layer\n",
        "    self.to(device)\n",
        "\n",
        "    input_channels = 1\n",
        "    out_channels = 50\n",
        "    min_kernel_size = 2\n",
        "    max_kernel_size = 7\n",
        "    width = embedding_dimension\n",
        "\n",
        "    # cnn\n",
        "    self.convs = nn.ModuleList([nn.Conv2d(input_channels, out_channels, kernel_size=(height, width)) for height in\n",
        "                                range(min_kernel_size, max_kernel_size + 1)])\n",
        "\n",
        "    for conv in self.convs:\n",
        "        init_weight_value = 6.0\n",
        "        init.xavier_normal_(conv.weight.data, gain=np.sqrt(init_weight_value))\n",
        "\n",
        "  def forward(self, sent_tokens):\n",
        "    # input: [s_nodes, max_sent_len]\n",
        "    enc_embed_input = self.sent_embedder(sent_tokens)  # [s_nodes, max_sent_len, dimension of initial embedding]\n",
        "\n",
        "    enc_conv_input = enc_embed_input.unsqueeze(1)  # [s_nodes, 1, max_sent_len, dimension of initial embedding]\n",
        "    enc_conv_output = [F.relu(conv(enc_conv_input)).squeeze(3) for conv in self.convs]  # for each kernel size shape (s_nodes, output_channel, max_sent_len-(kernel_size-1)\n",
        "    enc_maxpool_output = [F.max_pool1d(x, x.size(2)).squeeze(2) for x in enc_conv_output]  # for each kernel size shape (s_nodes, out_channel)\n",
        "    sent_embedding = torch.cat(enc_maxpool_output, 1)  # [s_nodes, 50 * 6]\n",
        "    return sent_embedding   # [s_nodes, 300]"
      ],
      "metadata": {
        "id": "OOOU_HKML5Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class H_encoder(nn.Module):\n",
        "  def __init__(self, embedding_layer, device = 'cpu', ouput_size = 264, embedding_size=300, hidden_size=256):\n",
        "    \"\"\"\n",
        "    H_encoder creates word, sentence and section inital embeddings for the GAT.\n",
        "\n",
        "    :param embedding_layer: layer that creates sentence initial embeddings\n",
        "    :param device: cpu or cuda\n",
        "    :param ouput_size: final embeddings size, must be common multiple of the attention heads for word, sentence and section, default 264\n",
        "    :param embedding_size: initial embeddings size, default 300\n",
        "    :param hidden_size: hidden size of LSTMs\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self._emb_lay = embedding_layer\n",
        "    self.word_projection = nn.Linear(embedding_size, ouput_size)\n",
        "\n",
        "    self.sent_encoder = sentence_cnn_encoder(embedding_layer, device)\n",
        "    self.BiLSTM = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, batch_first=True, bidirectional=True, num_layers=2, dropout=0.1)\n",
        "    self.lstm_projection = nn.Linear(hidden_size*2, ouput_size)\n",
        "\n",
        "    self.sentence_attention = nn.Linear(ouput_size, ouput_size)\n",
        "    self.sentence_context_vector = nn.Linear(ouput_size, 1, bias = False)\n",
        "    self.additional_embedding = nn.Linear(in_features = 1,out_features=64)\n",
        "    self.final_proj = nn.Linear(in_features=ouput_size+64, out_features=ouput_size)  #64 is the additional boundary feature embedding size\n",
        "    self.sec_BiLSTM = nn.LSTM(input_size=ouput_size, hidden_size=hidden_size, batch_first=True, bidirectional=True, num_layers=2, dropout=0.1)\n",
        "    self.sec_projection = nn.Linear(hidden_size*2, ouput_size)\n",
        "\n",
        "  def forward(self, graph): # graph is the batched graph\n",
        "    word_emb = self.set_word_embeddings(graph)\n",
        "    sent_emb = self.set_sentence_embedding(graph)\n",
        "    sec_emb = self.set_section_embedding(graph)\n",
        "    graph.ndata.pop('intermediate_embeddings')\n",
        "    return word_emb, sent_emb, sec_emb\n",
        "\n",
        "  def set_word_embeddings(self, graph): # graph is the batched graph\n",
        "    wnode_id = graph.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"]==0) # word nodes\n",
        "    wid = graph.nodes[wnode_id].data[\"id\"]  # [n_wnodes]\n",
        "    w_embed = self._emb_lay(wid)  # [n_wnodes, embedder_dimension]\n",
        "    w_embed = self.word_projection(w_embed) # [n_wnodes, ouput_size]\n",
        "    graph.nodes[wnode_id].data[\"initial_embeddings\"] = w_embed\n",
        "    return w_embed\n",
        "\n",
        "  def set_sentence_embedding(self, graph): # graph is the batched graph\n",
        "    snode_id = graph.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 1) # sentence nodes\n",
        "    cnn_feature = self.sent_encoder(graph.nodes[snode_id].data[\"words\"])\n",
        "    graph.nodes[snode_id].data[\"intermediate_embeddings\"] = cnn_feature # create intermediate embeddings using CNN\n",
        "\n",
        "    features, glen = self.get_sentence_features_and_len(graph, 1) # return the input format suitable for the pack_padded_sequence function with the sentences (semantic type = 1)\n",
        "    lstm_feature = self.sent_lstm_feature(features, glen, self.BiLSTM, self.lstm_projection) # sentence embeddings after BiLSTM\n",
        "\n",
        "    transposed_dist = graph.nodes[snode_id].data[\"boundary_dist\"].reshape(lstm_feature.size()[0], 1).type(torch.FloatTensor).to(device) # column vector containing the boundary distance feature of each sentence\n",
        "    custom = self.additional_embedding(transposed_dist) # integers are mapped to 64 dimentional embeddings\n",
        "\n",
        "    complete_embedding = torch.cat([lstm_feature, custom], 1)\n",
        "    initial_embedding = self.final_proj(complete_embedding) # concatenation reprojected in the same dimensions of other semantic nodes initial embeddings.\n",
        "    graph.nodes[snode_id].data[\"initial_embeddings\"] = initial_embedding\n",
        "\n",
        "    return initial_embedding\n",
        "\n",
        "  def set_section_embedding(self, graph): # graph is the batched graph\n",
        "    secnode_id = graph.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 2) # section nodes\n",
        "    node_feature_list = []\n",
        "    for Snode in secnode_id:\n",
        "        snodes = [nid for nid in graph.predecessors(Snode) if graph.nodes[nid].data[\"semantic_type\"]==1] # all sentence predecessors\n",
        "        sec_sent_feature = graph.nodes[snodes].data[\"initial_embeddings\"] # predecessors initial embeddings\n",
        "\n",
        "        # apply attention mechanism over sentence initial embeddings to obtain intermediate section embeddings\n",
        "        ui = self.sentence_attention(sec_sent_feature)\n",
        "        ui = torch.tanh(ui)\n",
        "        uiuatt = self.sentence_context_vector(ui)\n",
        "        max_value = uiuatt.max()  # scalar, for numerical stability during exponent calculation\n",
        "        alphas = F.softmax(uiuatt-max_value, dim = 0)\n",
        "        section = sec_sent_feature * alphas\n",
        "        section = section.sum(dim=0)\n",
        "        node_feature_list.append(section)\n",
        "\n",
        "    node_feature = torch.stack(node_feature_list)\n",
        "    graph.ndata.pop('intermediate_embeddings')\n",
        "    graph.nodes[secnode_id].data[\"intermediate_embeddings\"] = node_feature\n",
        "    features, glen = self.get_sentence_features_and_len(graph, 2)# return the input format suitable for the pack_padded_sequence function with the sections (semantic type = 2)\n",
        "    lstm_feature = self.sent_lstm_feature(features, glen, self.sec_BiLSTM, self.sec_projection) # section embeddings after BiLSTM\n",
        "    graph.nodes[secnode_id].data[\"initial_embeddings\"] = lstm_feature\n",
        "    return lstm_feature\n",
        "\n",
        "  def get_sentence_features_and_len(self, G, sem_type):\n",
        "    # returns 2 lists: \n",
        "    # *feature* contains the an intermediate representation for each sem_type node, \n",
        "    # *glen* contains the number of sem_type nodes for each unbatched graph\n",
        "    glist = dgl.unbatch(G)\n",
        "    feature = []\n",
        "    glen = []\n",
        "    for g in glist:\n",
        "        snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == sem_type)\n",
        "        feature.append(g.nodes[snode_id].data[\"intermediate_embeddings\"]) # list of lists where each element is an embedding vector\n",
        "        glen.append(len(snode_id))\n",
        "    return feature, glen\n",
        "\n",
        "  def sent_lstm_feature(self, features, glen, BiLSTM, lstm_projection):\n",
        "    # passes the sentence CNN embedding through the BiLSTM\n",
        "    pad_seq = pad_sequence(features, \n",
        "                            batch_first=True)\n",
        "    lstm_input = pack_padded_sequence(pad_seq, \n",
        "                                      glen, \n",
        "                                      batch_first=True, \n",
        "                                      enforce_sorted=False)\n",
        "    lstm_output, _ = BiLSTM(lstm_input) # takes a packed sequence in input and returns a packed sequence\n",
        "    lstm_feature = lstm_projection(lstm_output.data)  # [n_sent_nodes, embd_dim]\n",
        "    return lstm_feature"
      ],
      "metadata": {
        "id": "xzcPSKnPzFYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAT layers"
      ],
      "metadata": {
        "id": "3RM60e2yHLCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WSGATLayer(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    WSGATLayer word to sentence GAT.\n",
        "\n",
        "    :param in_dim: dimenion of embeddings in input\n",
        "    :param out_dim: output dimention of the linear layer\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "    self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "\n",
        "  def edge_attention(self, edges):\n",
        "    z2 = torch.cat([edges.src['wh'], edges.dst['wh']], dim=1)  # [edge_num, 2 * out_dim]\n",
        "    wa = F.leaky_relu(self.attn_fc(z2))  # [edge_num, 1]\n",
        "    return {'e': wa}\n",
        "\n",
        "  def message_func(self, edges):\n",
        "    return {'wh': edges.src['wh'], 'e': edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "    alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "    h = torch.sum(alpha * nodes.mailbox['wh'], dim=1)\n",
        "    return {'sh': h}\n",
        "\n",
        "  def forward(self, g, h):\n",
        "    wnode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 0) # words\n",
        "    snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 1) # sentences\n",
        "    wsedge_id = g.filter_edges(lambda edges: (edges.src[\"semantic_type\"] == 0) & (edges.dst[\"semantic_type\"] == 1) & (edges.data['edge_type'] == 0)) # word to sentence edges\n",
        "    wh = self.fc(h)\n",
        "    g.nodes[wnode_id].data['wh'] = wh[:len(wnode_id)]\n",
        "    g.nodes[snode_id].data['wh'] = wh[-len(snode_id):]\n",
        "    g.apply_edges(self.edge_attention, edges=wsedge_id)\n",
        "    g.pull(snode_id, self.message_func, self.reduce_func)\n",
        "    g.ndata.pop('wh')\n",
        "    h = g.ndata.pop('sh') # h is the weighted sum of the input embedding, where the weights are the attention coefficients\n",
        "    return h[snode_id]\n"
      ],
      "metadata": {
        "id": "iPMuQub3HKuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SWGATLayer(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    SWGATLayer sentence to word GAT.\n",
        "\n",
        "    :param in_dim: dimenion of embeddings in input\n",
        "    :param out_dim: output dimention of the linear layer\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "    self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "\n",
        "  def edge_attention(self, edges):\n",
        "    z2 = torch.cat([edges.src['wh'], edges.dst['wh']], dim=1)  # [edge_num, 2 * out_dim]\n",
        "    wa = F.leaky_relu(self.attn_fc(z2))  # [edge_num, 1]\n",
        "    return {'e': wa}\n",
        "\n",
        "  def message_func(self, edges):\n",
        "    return {'wh': edges.src['wh'], 'e': edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "    alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "    h = torch.sum(alpha * nodes.mailbox['wh'], dim=1)\n",
        "    return {'sh': h}\n",
        "\n",
        "  def forward(self, g, h):\n",
        "    wnode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 0) # word\n",
        "    snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 1) # sentence\n",
        "    swedge_id = g.filter_edges(lambda edges: (edges.src[\"semantic_type\"] == 1) & (edges.dst[\"semantic_type\"] == 0) & (edges.data['edge_type'] == 1)) # sentence to word edges\n",
        "    wh = self.fc(h)\n",
        "    g.nodes[wnode_id].data['wh'] = wh[:len(wnode_id)]\n",
        "    g.nodes[snode_id].data['wh'] = wh[-len(snode_id):]\n",
        "    g.apply_edges(self.edge_attention, edges=swedge_id)\n",
        "    g.pull(wnode_id, self.message_func, self.reduce_func)\n",
        "    g.ndata.pop('wh')\n",
        "    h = g.ndata.pop('sh') # h is the weighted sum of the input embedding, where the weights are the attention coefficients\n",
        "    return h[wnode_id]"
      ],
      "metadata": {
        "id": "d9-imSi7jUi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSGATLayer(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    SSGATLayer sentence to sentence GAT.\n",
        "\n",
        "    :param in_dim: dimenion of embeddings in input\n",
        "    :param out_dim: output dimention of the linear layer\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"    \n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "    self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "\n",
        "  def edge_attention(self, edges):\n",
        "    z2 = torch.cat([edges.src['wh'], edges.dst['wh']], dim=1)  # [edge_num, 2 * out_dim]\n",
        "    wa = F.leaky_relu(self.attn_fc(z2))  # [edge_num, 1]\n",
        "    return {'e': wa}\n",
        "\n",
        "  def message_func(self, edges):\n",
        "    return {'wh': edges.src['wh'], 'e': edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "    alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "    h = torch.sum(alpha * nodes.mailbox['wh'], dim=1)\n",
        "    return {'sh': h}\n",
        "\n",
        "  def forward(self, g, h):\n",
        "    snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 1) # sentence nodes\n",
        "    ssedge_id = g.filter_edges(lambda edges: edges.data['edge_type'] == 2) # intra-section sentence to sentence edges\n",
        "    wh = self.fc(h)\n",
        "    g.nodes[snode_id].data['wh'] = wh\n",
        "    g.apply_edges(self.edge_attention, edges=ssedge_id)\n",
        "    g.pull(snode_id, self.message_func, self.reduce_func)\n",
        "    g.ndata.pop('wh')\n",
        "    h = g.ndata.pop('sh') # h is the weighted sum of the input embedding, where the weights are the attention coefficients\n",
        "    return h[snode_id]\n"
      ],
      "metadata": {
        "id": "d1KDLGrJC9q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SecSGATLayer(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    SecSGATLayer section to sentence GAT.\n",
        "\n",
        "    :param in_dim: dimenion of embeddings in input\n",
        "    :param out_dim: output dimention of the linear layer\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"    \n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "    self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "\n",
        "  def edge_attention(self, edges):\n",
        "    z2 = torch.cat([edges.src['wh'], edges.dst['wh']], dim=1)  # [edge_num, 2 * out_dim]\n",
        "    wa = F.leaky_relu(self.attn_fc(z2))  # [edge_num, 1]\n",
        "    return {'e': wa}\n",
        "\n",
        "  def message_func(self, edges):\n",
        "    return {'wh': edges.src['wh'], 'e': edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "    alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "    h = torch.sum(alpha * nodes.mailbox['wh'], dim=1)\n",
        "    return {'sh': h}\n",
        "\n",
        "  def forward(self, g, h):\n",
        "    Snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 2) # section nodes\n",
        "    snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 1) # sentence nodes\n",
        "    Ssedge_id = g.filter_edges(lambda edges: (edges.src[\"semantic_type\"] == 2) & (edges.dst[\"semantic_type\"] == 1) & (edges.data['edge_type'] == 3)) # cross-section section to sentence edges\n",
        "    wh = self.fc(h)\n",
        "    g.nodes[Snode_id].data['wh'] = wh[:len(Snode_id)]\n",
        "    g.nodes[snode_id].data['wh'] = wh[-len(snode_id):]\n",
        "    g.apply_edges(self.edge_attention, edges=Ssedge_id)\n",
        "    g.pull(snode_id, self.message_func, self.reduce_func)\n",
        "    g.ndata.pop('wh')\n",
        "    h = g.ndata.pop('sh') # h is the weighted sum of the input embedding, where the weights are the attention coefficients\n",
        "    return h[snode_id]"
      ],
      "metadata": {
        "id": "88wp-uvSGdJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSecGATLayer(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    SSecGATLayer sentence to section GAT.\n",
        "\n",
        "    :param in_dim: dimenion of embeddings in input\n",
        "    :param out_dim: output dimention of the linear layer\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "    self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "\n",
        "  def edge_attention(self, edges):\n",
        "    z2 = torch.cat([edges.src['wh'], edges.dst['wh']], dim=1)  # [edge_num, 2 * out_dim]\n",
        "    wa = F.leaky_relu(self.attn_fc(z2))  # [edge_num, 1]\n",
        "    return {'e': wa}\n",
        "\n",
        "  def message_func(self, edges):\n",
        "    return {'wh': edges.src['wh'], 'e': edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "    alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "    h = torch.sum(alpha * nodes.mailbox['wh'], dim=1)\n",
        "    return {'sh': h}\n",
        "\n",
        "  def forward(self, g, h):\n",
        "    Snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 2) # section nodes\n",
        "    snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 1) # sentence nodes\n",
        "    sSedge_id = g.filter_edges(lambda edges: (edges.src[\"semantic_type\"] == 1) & (edges.dst[\"semantic_type\"] == 2) & (edges.data['edge_type'] == 4)) # intra-section sentence to section edges\n",
        "    wh = self.fc(h)\n",
        "    g.nodes[Snode_id].data['wh'] = wh[:len(Snode_id)]\n",
        "    g.nodes[snode_id].data['wh'] = wh[-len(snode_id):]\n",
        "    g.apply_edges(self.edge_attention, edges=sSedge_id)\n",
        "    g.pull(Snode_id, self.message_func, self.reduce_func)\n",
        "    g.ndata.pop('wh')\n",
        "    h = g.ndata.pop('sh') # h is the weighted sum of the input embedding, where the weights are the attention coefficients\n",
        "    return h[Snode_id]"
      ],
      "metadata": {
        "id": "k1PnE6jaJJIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SecSecGATLayer(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    SecSecGATLayer section to section GAT.\n",
        "\n",
        "    :param in_dim: dimenion of embeddings in input\n",
        "    :param out_dim: output dimention of the linear layer\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
        "    self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
        "\n",
        "  def edge_attention(self, edges):\n",
        "    z2 = torch.cat([edges.src['wh'], edges.dst['wh']], dim=1)  # [edge_num, 2 * out_dim]\n",
        "    wa = F.leaky_relu(self.attn_fc(z2))  # [edge_num, 1]\n",
        "    return {'e': wa}\n",
        "\n",
        "  def message_func(self, edges):\n",
        "    return {'wh': edges.src['wh'], 'e': edges.data['e']}\n",
        "\n",
        "  def reduce_func(self, nodes):\n",
        "    alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
        "    h = torch.sum(alpha * nodes.mailbox['wh'], dim=1)\n",
        "    return {'sh': h}\n",
        "\n",
        "  def forward(self, g, h):\n",
        "    Snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 2) # section nodes\n",
        "    SSedge_id = g.filter_edges(lambda edges: edges.data['edge_type'] == 5) # section to section edges\n",
        "    wh = self.fc(h)\n",
        "    g.nodes[Snode_id].data['wh'] = wh\n",
        "    g.apply_edges(self.edge_attention, edges=SSedge_id)\n",
        "    g.pull(Snode_id, self.message_func, self.reduce_func)\n",
        "    g.ndata.pop('wh')\n",
        "    h = g.ndata.pop('sh') # h is the weighted sum of the input embedding, where the weights are the attention coefficients\n",
        "    return h[Snode_id]"
      ],
      "metadata": {
        "id": "QNHA6k-GKfCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi head GAT\n"
      ],
      "metadata": {
        "id": "e3VwRV6yHHJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadGAT(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, num_heads, attn_drop_out, layer, device = 'cpu', merge='cat'):\n",
        "        \"\"\"\n",
        "        MultiHeadGAT merges all the heads attentions.\n",
        "\n",
        "        :param in_dim: dimenion of the GAT layer in input\n",
        "        :param out_dim: output dimention of the GAT layer in input\n",
        "        :param num_heads: number of heads for the GAT layer in input\n",
        "        :param attn_drop_out: dropout probability\n",
        "        :param layer: a specific GAT layer\n",
        "        :param device: cpu or cuda\n",
        "        :param merge: 'cut' or 'avg' if the heads are concatenated or averaged\n",
        "        \"\"\"\n",
        "        super(MultiHeadGAT, self).__init__()\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.heads.append(layer(in_dim, out_dim)) # list of num_heads GAT layers\n",
        "        self.merge = merge\n",
        "        self.dropout = nn.Dropout(attn_drop_out)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        head_outs = [attn_head(g, self.dropout(h)) for attn_head in self.heads]  # n_head * [n_nodes, out_dim]\n",
        "        if self.merge == 'cat':\n",
        "            # concat on the output feature dimension (dim=1)\n",
        "            result = torch.cat(head_outs, dim=1)  # [n_nodes, out_dim * n_head]\n",
        "        elif self.merge == 'avg':\n",
        "            # merge using average\n",
        "            result = torch.mean(torch.stack(head_outs))\n",
        "        return result"
      ],
      "metadata": {
        "id": "NqWXcChXIChV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete GAT"
      ],
      "metadata": {
        "id": "_HGLeAq8IE6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, num_heads, attn_drop_out, layerType, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    GAT runs the complete GAT mechanism.\n",
        "\n",
        "    :param in_dim: dimenion of the GAT layer in input\n",
        "    :param out_dim: output dimention of the GAT layer in input, must be a common multiple of the different num_head used\n",
        "    :param num_heads: number of heads for the GAT layer in input\n",
        "    :param attn_drop_out: dropout probability\n",
        "    :param layerType: which type of GAT layer is desired\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.layerType = layerType\n",
        "    self.to(device)\n",
        "    if layerType == \"W2S\":\n",
        "      self.layer = MultiHeadGAT(in_dim, int(out_dim / num_heads), num_heads, attn_drop_out, layer=WSGATLayer, device=device)\n",
        "    elif layerType == \"S2W\":\n",
        "      self.layer = MultiHeadGAT(in_dim, int(out_dim / num_heads), num_heads, attn_drop_out, layer=SWGATLayer, device=device)\n",
        "    elif layerType == \"S2S\":\n",
        "      self.layer = MultiHeadGAT(in_dim, int(out_dim / num_heads), num_heads, attn_drop_out, layer=SSGATLayer, device=device)\n",
        "    elif layerType == \"Sec2S\":\n",
        "      self.layer = MultiHeadGAT(in_dim, int(out_dim / num_heads), num_heads, attn_drop_out, layer=SecSGATLayer, device=device)\n",
        "    elif layerType == \"S2Sec\":\n",
        "      self.layer = MultiHeadGAT(in_dim, int(out_dim / num_heads), num_heads, attn_drop_out, layer=SSecGATLayer, device=device)\n",
        "    elif layerType == \"Sec2Sec\":\n",
        "      self.layer = MultiHeadGAT(in_dim, int(out_dim / num_heads), num_heads, attn_drop_out, layer=SecSecGATLayer, device=device)\n",
        "    else:\n",
        "      raise NotImplementedError(\"GAT Layer has not been implemented!\")\n",
        "\n",
        "\n",
        "  def forward(self, g, w, s):\n",
        "    if self.layerType == \"W2S\":\n",
        "      origin, neighbor = s, w\n",
        "      total = torch.cat([w,s])\n",
        "    elif self.layerType == \"S2W\":\n",
        "      origin, neighbor = w, s\n",
        "      total = torch.cat([w,s])\n",
        "    elif self.layerType == \"S2S\":\n",
        "      assert torch.equal(w, s) # check that w and s conincide in sentence to sentenc GAT\n",
        "      origin, total = w, s\n",
        "    elif self.layerType == \"Sec2S\": # w correspond to section\n",
        "      origin, neighbor = s, w\n",
        "      total = torch.cat([w,s])\n",
        "    elif self.layerType == \"S2Sec\": # w correspond to section\n",
        "      origin, neighbor = w, s\n",
        "      total = torch.cat([w,s])\n",
        "    elif self.layerType == \"Sec2Sec\":\n",
        "      assert torch.equal(w, s) # check that w and s conincide in section to section GAT\n",
        "      origin, total = w, s\n",
        "    else:\n",
        "        origin, neighbor = None, None\n",
        "    \n",
        "    h = F.elu(self.layer(g, total))\n",
        "    return h"
      ],
      "metadata": {
        "id": "J8oYHZaYGpFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusion"
      ],
      "metadata": {
        "id": "R7BGwxPeQ6ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class fusionLayer(nn.Module):\n",
        "  def __init__(self, embedding_size, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    fusionLayer performs the fusion operation.\n",
        "\n",
        "    :param embedding_size: size of the embeddings to be merged by means of fusion\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.lin = nn.Linear(2*embedding_size, embedding_size)\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    z = torch.cat([x, y], dim = 1)\n",
        "    z = self.lin(z)\n",
        "    z = torch.sigmoid(z)\n",
        "    fusion = z*x + (1-z)*y\n",
        "    return fusion"
      ],
      "metadata": {
        "id": "3fgs9eIGQ8yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position-wise Feed-Forward Network"
      ],
      "metadata": {
        "id": "HcFD31VuX3Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_in, d_hid, dropout=0.1, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    PositionwiseFeedForward applies a position wise feed forward neural network.\n",
        "\n",
        "    :param d_in: dimenion of the input embedding\n",
        "    :param d_hid: hidden dimension of the module\n",
        "    :param dropout: dropout probability, default 0.1\n",
        "    :param device: cpu or cuda\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self.w_1 = nn.Linear(d_in, d_hid)\n",
        "    self.w_2 = nn.Linear(d_hid, d_in)\n",
        "    self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.w_2(F.relu(self.w_1(x)))\n",
        "    x = self.dropout(x)\n",
        "    x += residual\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Gc10VTfNYA35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete extractive summarization module"
      ],
      "metadata": {
        "id": "5H7lBqk9GTf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtractiveSummarizazionModule(nn.Module):\n",
        "  def __init__(self, embedding_layer, num_heads1, num_heads2, num_iter, FFN_size, dropout, device = 'cpu', embedding_size = 264):\n",
        "    \"\"\"\n",
        "    ExtractiveSummarizazionModule runs the complete GAT mechanism.\n",
        "\n",
        "    :param embedding_layer: embedding layer used by the H0 encoder\n",
        "    :param num_heads1: num heads for the word GAT\n",
        "    :param num_heads2: number of heads for the sentence and section GATs\n",
        "    :param num_iter: corresponds to the T parameter, is the number of iteative updates of each node representation\n",
        "    :param FFN_size: hidden dimention of the position-wise FFN\n",
        "    :param dropout: dropout probability\n",
        "    :param device: cpu or cuda\n",
        "    :param embedding_size: must be a common multiple of num_heads1 and num_heads2\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.to(device)\n",
        "    self.H0 = H_encoder(embedding_layer, device)\n",
        "    self.T = num_iter\n",
        "\n",
        "    self.word2sentGat = GAT(embedding_size, embedding_size, num_heads1, dropout, \"W2S\", device)\n",
        "    self.sent2wordGat = GAT(embedding_size, embedding_size, num_heads2, dropout, \"S2W\", device)\n",
        "    self.sent2sentGat = GAT(embedding_size, embedding_size, num_heads2, dropout, \"S2S\", device)\n",
        "    self.sec2sentGat = GAT(embedding_size, embedding_size, num_heads2, dropout, \"Sec2S\", device)\n",
        "    self.sent2secGat = GAT(embedding_size, embedding_size, num_heads2, dropout, \"S2Sec\", device)\n",
        "    self.sec2secGat = GAT(embedding_size, embedding_size, num_heads2, dropout, \"Sec2Sec\", device)\n",
        "\n",
        "    self.ws_Ss_fusion = fusionLayer(embedding_size, device)\n",
        "    self.prevFus_ss_fusion = fusionLayer(embedding_size, device)\n",
        "    self.sS_SS_fusion = fusionLayer(embedding_size, device)\n",
        "\n",
        "    self.FFN_word = PositionwiseFeedForward(embedding_size, FFN_size, dropout, device)\n",
        "    self.FFN_sent = PositionwiseFeedForward(embedding_size, FFN_size, dropout, device)\n",
        "    self.FFN_sec = PositionwiseFeedForward(embedding_size, FFN_size, dropout, device)\n",
        "\n",
        "    self.classification_layer = nn.Linear(embedding_size, 2)\n",
        "    \n",
        "\n",
        "  def forward(self, graph):\n",
        "    word_emb, sent_emb, sec_emb = self.H0(graph) # set initial embeddings\n",
        "\n",
        "    word_state = word_emb\n",
        "    sent_state = sent_emb\n",
        "    sec_state = sec_emb\n",
        "\n",
        "    for _ in range(self.T):\n",
        "      U_s2w = self.sent2wordGat(graph, word_state, sent_state) # U_s2w has shape (num_words in batched graph, embedding_size)\n",
        "      h_w2s = self.word2sentGat(graph, word_state, sent_state) # h_w2s has shape (num_sentences in batched graph, embedding_size)\n",
        "      h_s2s = self.sent2sentGat(graph, sent_state, sent_state) # h_s2s has shape (num_sentences in batched graph, embedding_size)\n",
        "      h_sec2s = self.sec2sentGat(graph, sec_state, sent_state) # h_sec2s has shape (num_sentences in batched graph, embedding_size)\n",
        "      h_s2sec = self.sent2secGat(graph, sec_state, sent_state) # h_s2sec has shape (num_sections in batched graph, embedding_size)\n",
        "      h_sec2sec = self.sec2secGat(graph, sec_state, sec_state) # h_sec2sec has shape (num_sections in batched graph, embedding_size)\n",
        "      \n",
        "      U_s = self.prevFus_ss_fusion(self.ws_Ss_fusion(h_w2s, h_sec2s), h_s2s) # shape (num_sentences in batched graph, embedding_size)\n",
        "      U_Sec = self.sS_SS_fusion(h_s2sec, h_sec2sec) # shape (num_sections in batched graph, embedding_size)\n",
        "\n",
        "      word_state = self.FFN_word(U_s2w + word_state) # shape (num_words in batched graph, embedding_size)\n",
        "      sent_state = self.FFN_sent(U_s + sent_state) # shape (num_sentences in batched graph, embedding_size)\n",
        "      sec_state = self.FFN_sec(U_Sec + sec_state) # shape (num_sections in batched graph, embedding_size)\n",
        "\n",
        "    probs = torch.sigmoid(self.classification_layer(sent_state))\n",
        "    return probs # returns for each sentence 2 probabilities"
      ],
      "metadata": {
        "id": "wmEOW_iGDbqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tester"
      ],
      "metadata": {
        "id": "o662HVz1BL-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tester():\n",
        "  def __init__(self, model, n_max, dir = None):\n",
        "    \"\"\"\n",
        "    Tester evaluates the model and returns a file containing extracted and reference summaries when dir != None\n",
        "\n",
        "        :param model: the model\n",
        "        :param n_max: maximum number of words in summary\n",
        "        :param dir: for saving decode files\n",
        "    \"\"\"\n",
        "    self.model = model\n",
        "    self.n_max = n_max\n",
        "    self.save_dir = dir\n",
        "    self.extracts = []\n",
        "\n",
        "    self.batch_number = 0\n",
        "    self.running_loss = 0\n",
        "    self.example_num = 0\n",
        "    self.total_sentence_num = 0\n",
        "    self.rougePairNum = 0\n",
        "\n",
        "    self.hypothesis = []\n",
        "    self.reference = []\n",
        "\n",
        "    self.pred, self.true, self.match, self.match_true = 0, 0, 0, 0\n",
        "    self._F = 0\n",
        "    self.criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "  def SaveDecodeFile(self):\n",
        "    import datetime\n",
        "    nowTime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    log_dir = os.path.join(self.save_dir, nowTime)\n",
        "    with open(log_dir, \"wb\") as resfile:\n",
        "      for i in range(self.rougePairNum):\n",
        "        resfile.write(b\"[Reference]\\t\")\n",
        "        resfile.write(self.reference[i].encode('utf-8'))\n",
        "        resfile.write(b\"\\n\")\n",
        "        resfile.write(b\"[Hypothesis]\\t\")\n",
        "        resfile.write(self.hypothesis[i].encode('utf-8'))\n",
        "        resfile.write(b\"\\n\")\n",
        "        resfile.write(b\"\\n\")\n",
        "        resfile.write(b\"\\n\")\n",
        "    \n",
        "  def running_avg_loss(self):\n",
        "    return self.running_loss / self.batch_number\n",
        "\n",
        "  def evaluation(self, G, index, dataset):\n",
        "    \"\"\"\n",
        "      :param G: the batched graph\n",
        "      :param index: list containing the index of each graph in the batched graph\n",
        "      :param dataset: dataset which includes text and summary\n",
        "    \"\"\"\n",
        "    self.batch_number += 1\n",
        "    outputs = self.model.forward(G)\n",
        "    snode_id = G.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 1) # sentence nodes\n",
        "    label = G.ndata[\"label\"][snode_id] # [sent_nodes]\n",
        "    G.nodes[snode_id].data[\"loss\"] = self.criterion(outputs, label).unsqueeze(-1) # [sent_nodes, 1] loss of each sentence\n",
        "    loss = dgl.sum_nodes(G, \"loss\")  # [batch_size, 1] sums the losses of all the sentences of a single graph\n",
        "    loss = loss.mean() # mean of the losses of each graph\n",
        "    self.running_loss += float(loss.data)\n",
        "    G.nodes[snode_id].data[\"p\"] = outputs\n",
        "    glist = dgl.unbatch(G)\n",
        "    for j in range(len(glist)): # each unbatched graph is a document\n",
        "      original_article_sents = []\n",
        "      idx = index[j]\n",
        "      example = dataset.get_doc(idx)\n",
        "      for sec in example.sections:\n",
        "        original_article_sents.extend([sent.sentence for sent in sec.sentences])\n",
        "      sent_max_number = len(original_article_sents)\n",
        "      reference = example.abstract.all_sentences\n",
        "\n",
        "      g = glist[j]\n",
        "      snode_id = g.filter_nodes(lambda nodes: nodes.data[\"semantic_type\"] == 1) # sentence nodes of a document\n",
        "      N = len(snode_id)\n",
        "      p_sent = g.ndata[\"p\"][snode_id] # [snode, 2]\n",
        "      label = g.ndata[\"label\"][snode_id].squeeze().cpu()   # [n_node]\n",
        "      num_words = g.ndata[\"num_words\"][snode_id]\n",
        "      topk, pred_idx = torch.topk(p_sent[:,1], N) # order the sentences on the basis of their probability to be extracted (p[:,1])\n",
        "      lens = 0\n",
        "      hyps = ''\n",
        "      for id in pred_idx:\n",
        "        lens += num_words[id]\n",
        "        if lens < self.n_max: # if adding the new sentence we are still under the limit of self.n_max word the sentence is concatenated\n",
        "          if id < sent_max_number:\n",
        "            hyps = hyps + original_article_sents[id] # the order of the original document is not preserved\n",
        "      self.hypothesis.append(hyps)\n",
        "      self.reference.append(reference)\n",
        "      self.rougePairNum += 1"
      ],
      "metadata": {
        "id": "58mXQiYyBPAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define training parameter for extractive summarization"
      ],
      "metadata": {
        "id": "mmKxAFZBpM9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ES_model_path = '/content/drive/MyDrive/ES_model_arx_pub.pkl'\n",
        "ES_batch_size = 25\n",
        "lr = 1e-4\n",
        "epochs = 5\n",
        "max_grad_norm = 2\n",
        "workers = 2\n",
        "num_heads_wordGat = 6\n",
        "num_heads_sent_secGat = 8\n",
        "T = 2\n",
        "FFN_hidden_size = 2048\n",
        "dropout = 0.1\n",
        "output_size = 264 # must be a common multiple of the attention heads\n",
        "max_num_words_in_summary = 200\n",
        "best_rouge = True # True to save the model the obtains the best rouge score against the golden summary, False to save the model which reaches the lowest cross entropy loss\n",
        "embedder = embedding_layer(tokenizer, 300, device)\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "model = ExtractiveSummarizazionModule(embedder, num_heads_wordGat, num_heads_sent_secGat, T, FFN_hidden_size, dropout, device, output_size)\n",
        "model.to(device)\n",
        "train_loader = torch.utils.data.DataLoader(digested_docs, batch_size=ES_batch_size, shuffle=True, num_workers=workers,collate_fn=graph_collate_fn)\n",
        "eval_loader = torch.utils.data.DataLoader(eval_digested_docs, batch_size=ES_batch_size, shuffle=True, num_workers=workers,collate_fn=graph_collate_fn)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, betas = [0.9, 0.999], eps=1e-8)"
      ],
      "metadata": {
        "id": "TnkCmFsxiCn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training of extractive summarization module"
      ],
      "metadata": {
        "id": "o29w7E05gEQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_eval_loss = np.float('inf')\n",
        "best_eval_rouge = 0.0\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Training\n",
        "  training_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch, (batched_graph, index) in enumerate(train_loader):\n",
        "    batched_graph = batched_graph.to(device)\n",
        "    predictions = model(batched_graph)\n",
        "    sent_nodes = batched_graph.filter_nodes(lambda node: node.data['semantic_type'] == 1) # sentence nodes\n",
        "    labels = batched_graph.nodes[sent_nodes].data['label']\n",
        "    batched_graph.nodes[sent_nodes].data[\"loss\"] = criterion(predictions, labels).unsqueeze(-1)  # [n_nodes, 1], cross entropy loss between probabilities and ground truth labels\n",
        "    loss = dgl.sum_nodes(batched_graph, \"loss\")  # [batch_size, 1]\n",
        "    loss = loss.mean()\n",
        "    print(f'Batch/Epoch {batch}/{epochs}. Batch loss: {loss:.3f}.')\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    training_loss += float(loss.data)\n",
        "\n",
        "  # Evaluation, at each epoch\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      tester = Tester(model, max_num_words_in_summary)\n",
        "      for i, (G, index) in enumerate(eval_loader):\n",
        "        G = G.to(device)\n",
        "        tester.evaluation(G, index, eval_digested_docs)\n",
        "\n",
        "  running_avg_loss = tester.running_avg_loss\n",
        "  rouge = Rouge()\n",
        "  scores_all = rouge.get_scores(tester.hypothesis, tester.reference, avg=True)\n",
        "  avg_rouge = np.mean([scores_all['rouge-1']['f'], scores_all['rouge-2']['f'], scores_all['rouge-l']['f']])# mean of all ROUGE f1 score\n",
        "  if best_rouge: # save the model that reaches the highest ROUGE score on the validation set\n",
        "    if avg_rouge > best_eval_rouge:\n",
        "      print(\"Save model with higher rouge score. Average F rouge score: \", avg_rouge, \"Previous best avg rouge score: \", best_eval_rouge )\n",
        "      best_eval_rouge = avg_rouge\n",
        "      save_model(model, ES_model_path)\n",
        "  else: # save the model that reaches the lowest loss\n",
        "    if running_avg_loss < best_eval_loss:\n",
        "      print(\"Save model with lower loss. Evaluation loss: \", running_avg_loss)\n",
        "      best_eval_loss = running_avg_loss\n",
        "      save_model(model, ES_model_path)\n",
        "\n",
        "  print(f'Epoch {epoch}. Training loss: {training_loss:.3f}.')"
      ],
      "metadata": {
        "id": "-GeIt0CdhuzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "zIsnocnhbyi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_datapath = '/content/drive/MyDrive/arx_pub-dataset/test_subset.txt'\n",
        "test_results = './test' # directory for the document containing reference and extracted summary for each test document\n",
        "test_batch_size = 10"
      ],
      "metadata": {
        "id": "XVHqimlAd3xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CR_model = load_model(ContentRankingModule(tokenizer=tokenizer, device=device), CR_model_path)\n",
        "ES_model = load_model(ExtractiveSummarizazionModule(embedder, num_heads_wordGat, num_heads_sent_secGat, T, FFN_hidden_size, dropout, device, output_size), ES_model_path)\n",
        "ES_model.to(device)\n",
        "test_docs = ScientificPapaerDataset(test_datapath, max_sent_len, tokenizer)"
      ],
      "metadata": {
        "id": "1ZrlCSDkcNkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_section_scores, test_sentence_scores = predict(CR_model, test_docs, test_batch_size, workers)\n",
        "test_top_sections_ids, test_top_sentences_ids = select_top_sections_and_sentences(test_section_scores, test_sentence_scores, m, n)"
      ],
      "metadata": {
        "id": "O41C6I1OfZ0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_digested_docs = DigestedDataset(test_docs, test_top_sections_ids, test_top_sentences_ids)"
      ],
      "metadata": {
        "id": "on-u-ja4fh_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ES_predict(model, data, ES_batch_size, max_num_words_in_summary, test_results):\n",
        "  \"\"\"\n",
        "  ES_predict extracts and evaluates the summaries for the text documents\n",
        "\n",
        "      :param model: the Extractive Summarization Module\n",
        "      :param data: documents in input\n",
        "      :param ES_batch_size: batch size for the internal dataloader\n",
        "      :param max_num_words_in_summary: maximum word length of the extracted summary\n",
        "      :param test_results: directory in which to store extracted and reference summary of each document in input\n",
        "  \"\"\"\n",
        "  if not os.path.exists(test_results) : os.makedirs(test_results)\n",
        "  model.eval()\n",
        "  dataloader = torch.utils.data.DataLoader(data, batch_size=ES_batch_size, num_workers=4, collate_fn=graph_collate_fn)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    tester = Tester(model, max_num_words_in_summary, test_results)\n",
        "\n",
        "    for batched_graph, index in dataloader:\n",
        "      batched_graph = batched_graph.to(device)\n",
        "      tester.evaluation(batched_graph, index, data)\n",
        "\n",
        "  running_avg_loss = tester.running_avg_loss\n",
        "  rouge = Rouge()\n",
        "  scores_all = rouge.get_scores(tester.hypothesis, tester.reference, avg=True)\n",
        "\n",
        "  res = \"Rouge1:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\" % (scores_all['rouge-1']['p'], scores_all['rouge-1']['r'], scores_all['rouge-1']['f']) \\\n",
        "          + \"Rouge2:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\" % (scores_all['rouge-2']['p'], scores_all['rouge-2']['r'], scores_all['rouge-2']['f']) \\\n",
        "              + \"Rougel:\\n\\tp:%.6f, r:%.6f, f:%.6f\\n\" % (scores_all['rouge-l']['p'], scores_all['rouge-l']['r'], scores_all['rouge-l']['f'])\n",
        "  print(res)\n",
        "  tester.SaveDecodeFile()"
      ],
      "metadata": {
        "id": "KWcbGP_adT5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ES_predict(ES_model, test_digested_docs, ES_batch_size, max_num_words_in_summary, test_results)"
      ],
      "metadata": {
        "id": "BOZCLGPZiXTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation study"
      ],
      "metadata": {
        "id": "69Ls8TivxwwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disable content ranking module"
      ],
      "metadata": {
        "id": "SBlr1Seu1Fdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_lead_digests(section_scores, sentence_scores, m, n): # select the ids of the m lead sections and  n lead sentences\n",
        "  sec_lead_ids = np.tile(range(m), (section_scores.shape[0], 1))\n",
        "  sent_lead_ids = np.tile(range(n), (sentence_scores.shape[0], sentence_scores.shape[1], 1))\n",
        "  return sec_lead_ids, sent_lead_ids"
      ],
      "metadata": {
        "id": "mPh9W0txx47y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sec_lead_ids, sent_lead_ids = select_lead_digests(test_section_scores, test_sentence_scores, m, n)"
      ],
      "metadata": {
        "id": "E36tPLJqyd6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lead_test_digested_docs = DigestedDataset(test_docs, sec_lead_ids, sent_lead_ids)"
      ],
      "metadata": {
        "id": "sb3IFgqq2Zvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ES_predict(ES_model, lead_test_digested_docs, ES_batch_size, max_num_words_in_summary, test_results)"
      ],
      "metadata": {
        "id": "cFoLCWb02ucR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disable iteative update in extractive summarization module"
      ],
      "metadata": {
        "id": "mwb2-BO521MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = 1\n",
        "ES_model_path_T1 = '/content/drive/MyDrive/ES_model_T1.pkl'\n",
        "model = ExtractiveSummarizazionModule(embedder, num_heads_wordGat, num_heads_sent_secGat, T, FFN_hidden_size, dropout, device, output_size)\n",
        "model.to(device)\n",
        "T = 2 # for future instanciations of the model"
      ],
      "metadata": {
        "id": "tqbfIoVn8HlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New training of the extractive summarization module"
      ],
      "metadata": {
        "id": "Ruqubylo8xur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_eval_loss = np.float('inf')\n",
        "best_eval_rouge = 0.0\n",
        "for epoch in range(epochs):\n",
        "  training_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch, (batched_graph, index) in enumerate(train_loader):\n",
        "    batched_graph = batched_graph.to(device)\n",
        "    predictions = model(batched_graph)\n",
        "    sent_nodes = batched_graph.filter_nodes(lambda node: node.data['semantic_type'] == 1)\n",
        "    labels = batched_graph.nodes[sent_nodes].data['label']\n",
        "    batched_graph.nodes[sent_nodes].data[\"loss\"] = criterion(predictions, labels).unsqueeze(-1)  # [n_nodes, 1]\n",
        "    loss = dgl.sum_nodes(batched_graph, \"loss\")  # [batch_size, 1]\n",
        "    loss = loss.mean()\n",
        "    print(f'Batch/Epoch {batch}/{epochs}. Batch loss: {loss:.3f}.')\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    training_loss += float(loss.data)\n",
        "\n",
        "  # Evaluation\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      tester = Tester(model, max_num_words_in_summary)\n",
        "      for i, (G, index) in enumerate(eval_loader):\n",
        "        G = G.to(device)\n",
        "        tester.evaluation(G, index, eval_digested_docs)\n",
        "\n",
        "  running_avg_loss = tester.running_avg_loss\n",
        "  rouge = Rouge()\n",
        "  scores_all = rouge.get_scores(tester.hypothesis, tester.reference, avg=True)\n",
        "  avg_rouge = np.mean([scores_all['rouge-1']['f'], scores_all['rouge-2']['f'], scores_all['rouge-l']['f']])\n",
        "  if best_rouge:\n",
        "    if avg_rouge > best_eval_rouge:\n",
        "      print(\"Save model with higher rouge score. Average F rouge score: \", avg_rouge, \"Previous best avg rouge score: \", best_eval_rouge )\n",
        "      best_eval_rouge = avg_rouge\n",
        "      save_model(model, ES_model_path_T1)\n",
        "  else:\n",
        "    if running_avg_loss < best_eval_loss:\n",
        "      print(\"Save model with lower loss. Evaluation loss: \", running_avg_loss)\n",
        "      best_eval_loss = running_avg_loss\n",
        "      save_model(model, ES_model_path_T1)\n",
        "\n",
        "  print(f'Epoch {epoch}. Training loss: {training_loss:.3f}.')"
      ],
      "metadata": {
        "id": "d_cgYGrk8wgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ES_model_t1 = load_model(ExtractiveSummarizazionModule(embedder, num_heads_wordGat, num_heads_sent_secGat, T, FFN_hidden_size, dropout, device, output_size), ES_model_path_T1)\n",
        "ES_model_t1.to(device)"
      ],
      "metadata": {
        "id": "Mw8h-u4N9SCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ES_predict(ES_model_t1, test_digested_docs, ES_batch_size, max_num_words_in_summary, test_results)"
      ],
      "metadata": {
        "id": "gdUh_kOD9zdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disable boundary distance feature\n",
        "Set it equal to 1 for each sentence"
      ],
      "metadata": {
        "id": "eLEYE7HC_nZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ES_model_path_no_bd = '/content/drive/MyDrive/ES_model_no_bd.pkl'\n",
        "model = ExtractiveSummarizazionModule(embedder, num_heads_wordGat, num_heads_sent_secGat, T, FFN_hidden_size, dropout, device, output_size)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "IX_CqviQAW83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_bd_digested_docs = DigestedDataset(documents, top_sections_ids, top_sentences_ids, m, n, disable_bound_dist = True)\n",
        "no_bd_eval_digested_docs = DigestedDataset(eval_docs, eval_top_sections_ids, eval_top_sentences_ids, m, n, disable_bound_dist = True)"
      ],
      "metadata": {
        "id": "YqrSmCvV_uVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(no_bd_digested_docs, batch_size=ES_batch_size, shuffle=True, num_workers=workers,collate_fn=graph_collate_fn)\n",
        "eval_loader = torch.utils.data.DataLoader(no_bd_eval_digested_docs, batch_size=ES_batch_size, shuffle=True, num_workers=workers,collate_fn=graph_collate_fn)"
      ],
      "metadata": {
        "id": "FEthHgMOAHnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_eval_loss = np.float('inf')\n",
        "best_eval_rouge = 0.0\n",
        "for epoch in range(epochs):\n",
        "  training_loss = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch, (batched_graph, index) in enumerate(train_loader):\n",
        "    batched_graph = batched_graph.to(device)\n",
        "    predictions = model(batched_graph)\n",
        "    sent_nodes = batched_graph.filter_nodes(lambda node: node.data['semantic_type'] == 1)\n",
        "    labels = batched_graph.nodes[sent_nodes].data['label']\n",
        "    batched_graph.nodes[sent_nodes].data[\"loss\"] = criterion(predictions, labels).unsqueeze(-1)  # [n_nodes, 1]\n",
        "    loss = dgl.sum_nodes(batched_graph, \"loss\")  # [batch_size, 1]\n",
        "    loss = loss.mean()\n",
        "    print(f'Batch/Epoch {batch}/{epochs}. Batch loss: {loss:.3f}.')\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    training_loss += float(loss.data)\n",
        "\n",
        "  # Evaluation\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      tester = Tester(model, max_num_words_in_summary)\n",
        "      for i, (G, index) in enumerate(eval_loader):\n",
        "        G = G.to(device)\n",
        "        tester.evaluation(G, index, eval_digested_docs)\n",
        "\n",
        "  running_avg_loss = tester.running_avg_loss\n",
        "  rouge = Rouge()\n",
        "  scores_all = rouge.get_scores(tester.hypothesis, tester.reference, avg=True)\n",
        "  avg_rouge = np.mean([scores_all['rouge-1']['f'], scores_all['rouge-2']['f'], scores_all['rouge-l']['f']])\n",
        "  if best_rouge:\n",
        "    if avg_rouge > best_eval_rouge:\n",
        "      print(\"Save model with higher rouge score. Average F rouge score: \", avg_rouge, \"Previous best avg rouge score: \", best_eval_rouge )\n",
        "      best_eval_rouge = avg_rouge\n",
        "      save_model(model, ES_model_path_no_bd)\n",
        "  else:\n",
        "    if running_avg_loss < best_eval_loss:\n",
        "      print(\"Save model with lower loss. Evaluation loss: \", running_avg_loss)\n",
        "      best_eval_loss = running_avg_loss\n",
        "      save_model(model, ES_model_path_no_bd)\n",
        "\n",
        "  print(f'Epoch {epoch}. Training loss: {training_loss:.3f}.')"
      ],
      "metadata": {
        "id": "cJJdqQc6AgAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ES_model_no_bd = load_model(ExtractiveSummarizazionModule(embedder, num_heads_wordGat, num_heads_sent_secGat, T, FFN_hidden_size, dropout, device, output_size), ES_model_path_no_bd)\n",
        "ES_model_no_bd.to(device)"
      ],
      "metadata": {
        "id": "vZ1oMvfzA4Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_digested_docs_no_bd = DigestedDataset(test_docs, test_top_sections_ids, test_top_sentences_ids, disable_bound_dist = True)"
      ],
      "metadata": {
        "id": "Kp-uJSRxBPGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ES_predict(ES_model_no_bd, test_digested_docs, ES_batch_size, max_num_words_in_summary, test_results)"
      ],
      "metadata": {
        "id": "zQICSOc0Bl1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}